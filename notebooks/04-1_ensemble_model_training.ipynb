{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블 모델 학습 및 평가 (Ensemble Model Training & Evaluation)\n",
    "\n",
    "## 목표\n",
    "- XGBoost와 LightGBM을 결합한 앙상블 모델 구현\n",
    "- 다양한 앙상블 기법 비교 (Averaging, Weighted, Voting, Stacking)\n",
    "- 최적의 앙상블 모델 선정 및 성능 향상 확인\n",
    "\n",
    "## 작업 내역\n",
    "1. 환경 설정 및 데이터 로드\n",
    "2. 기존 학습된 모델 로드 (XGBoost, LightGBM)\n",
    "3. Simple Averaging Ensemble\n",
    "4. Weighted Ensemble (성능 기반)\n",
    "5. Optimized Weighted Ensemble\n",
    "6. Voting Ensemble (Soft/Hard)\n",
    "7. Stacking Ensemble\n",
    "8. 모델 성능 비교 및 시각화\n",
    "9. 앙상블 분석 (가중치, 예측 차이, Hard cases)\n",
    "10. 최종 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 머신러닝 라이브러리\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve, auc,\n",
    "    precision_score, recall_score, f1_score, average_precision_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# 모델\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# 출력 설정\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 모듈 임포트\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pipeline.models import EnsembleModel, VotingEnsemble\n",
    "from pipeline.evaluation import (\n",
    "    ModelEvaluator,\n",
    "    calculate_metrics,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"Custom modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering 완료된 데이터 로드\n",
    "data_path = Path('../data/processed/featured_data.csv')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"Warning: featured_data.csv not found. Using preprocessed_data.csv instead.\")\n",
    "    data_path = Path('../data/processed/preprocessed_data.csv')\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {len(df.columns)}\")\n",
    "print(f\"Rows: {len(df):,}\")\n",
    "print(f\"Date range: {df['TA_YM'].min()} ~ {df['TA_YM'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 변수 및 제외 컬럼 정의\n",
    "target_col = 'is_closed'\n",
    "\n",
    "exclude_cols = [\n",
    "    'ENCODED_MCT',\n",
    "    'TA_YM',\n",
    "    'is_closed',\n",
    "    'will_close_1m',\n",
    "    'will_close_3m',\n",
    "    'months_until_close',\n",
    "    'MCT_ME_D',\n",
    "    'MCT_BSE_AR',\n",
    "    'MCT_NM',\n",
    "    'MCT_BRD_NUM',\n",
    "    'MCT_SIGUNGU_NM',\n",
    "    'HPSN_MCT_ZCD_NM',\n",
    "    'HPSN_MCT_BZN_CD_NM',\n",
    "    'ARE_D',\n",
    "]\n",
    "\n",
    "# Feature 컬럼 선택\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "feature_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y 분리\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# 결측값 처리 (중앙값으로 대체)\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(f\"Filling {X.isnull().sum().sum()} missing values with median...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Positive rate: {y.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test Split (시계열 고려)\n",
    "df_with_features = df[['TA_YM', target_col]].join(X)\n",
    "\n",
    "# Train: 2023 (202301 ~ 202312)\n",
    "train_mask = (df_with_features['TA_YM'] >= 202301) & (df_with_features['TA_YM'] <= 202312)\n",
    "\n",
    "# Validation: 2024 Jan-Jun (202401 ~ 202406)\n",
    "val_mask = (df_with_features['TA_YM'] >= 202401) & (df_with_features['TA_YM'] <= 202406)\n",
    "\n",
    "# Test: 2024 Jul-Dec (202407 ~ 202412)\n",
    "test_mask = (df_with_features['TA_YM'] >= 202407) & (df_with_features['TA_YM'] <= 202412)\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "\n",
    "X_val = X[val_mask]\n",
    "y_val = y[val_mask]\n",
    "\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(\"Data split completed:\")\n",
    "print(f\"\\nTrain: {X_train.shape[0]:,} samples, Positive: {y_train.sum():,} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Val:   {X_val.shape[0]:,} samples, Positive: {y_val.sum():,} ({y_val.mean()*100:.2f}%)\")\n",
    "print(f\"Test:  {X_test.shape[0]:,} samples, Positive: {y_test.sum():,} ({y_test.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 기존 학습된 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 디렉토리\n",
    "model_dir = Path('../models')\n",
    "\n",
    "# XGBoost (Tuned) 모델 로드\n",
    "with open(model_dir / 'xgboost_best.pkl', 'rb') as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "print(\"Loaded: XGBoost (Tuned)\")\n",
    "\n",
    "# LightGBM (Baseline) 모델 로드\n",
    "with open(model_dir / 'lightgbm_baseline.pkl', 'rb') as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "print(\"Loaded: LightGBM (Baseline)\")\n",
    "\n",
    "print(\"\\nModels loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 성능 확인 (Validation)\n",
    "print(\"=\"*80)\n",
    "print(\"Baseline Model Performance - Validation Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# XGBoost\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "y_val_pred_proba_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"\\nXGBoost (Tuned):\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_xgb):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_xgb):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_xgb):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_xgb):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_xgb):.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "y_val_pred_lgb = lgb_model.predict(X_val)\n",
    "y_val_pred_proba_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"\\nLightGBM (Baseline):\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_lgb):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_lgb):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_lgb):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_lgb):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_lgb):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 성능 확인 (Test)\n",
    "print(\"=\"*80)\n",
    "print(\"Baseline Model Performance - Test Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# XGBoost\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "y_test_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nXGBoost (Tuned):\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_xgb):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_xgb):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_xgb):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_xgb):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_xgb):.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "y_test_pred_lgb = lgb_model.predict(X_test)\n",
    "y_test_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nLightGBM (Baseline):\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_lgb):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_lgb):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_lgb):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_lgb):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_lgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Averaging Ensemble\n",
    "\n",
    "가장 간단한 앙상블 방법: 두 모델의 예측 확률을 단순 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Averaging Ensemble\n",
    "y_val_pred_proba_avg = (y_val_pred_proba_xgb + y_val_pred_proba_lgb) / 2\n",
    "y_val_pred_avg = (y_val_pred_proba_avg >= 0.5).astype(int)\n",
    "\n",
    "y_test_pred_proba_avg = (y_test_pred_proba_xgb + y_test_pred_proba_lgb) / 2\n",
    "y_test_pred_avg = (y_test_pred_proba_avg >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Simple Averaging Ensemble\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_avg):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_avg):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_avg):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_avg):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_avg):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_avg):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_avg):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_avg):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_avg):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_avg):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Weighted Ensemble (성능 기반)\n",
    "\n",
    "Validation set의 ROC-AUC 성능에 비례하는 가중치 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation 성능 기반 가중치 계산\n",
    "roc_auc_xgb = roc_auc_score(y_val, y_val_pred_proba_xgb)\n",
    "roc_auc_lgb = roc_auc_score(y_val, y_val_pred_proba_lgb)\n",
    "\n",
    "weight_xgb = roc_auc_xgb / (roc_auc_xgb + roc_auc_lgb)\n",
    "weight_lgb = roc_auc_lgb / (roc_auc_xgb + roc_auc_lgb)\n",
    "\n",
    "print(f\"Performance-based weights:\")\n",
    "print(f\"  XGBoost: {weight_xgb:.4f} (ROC-AUC: {roc_auc_xgb:.4f})\")\n",
    "print(f\"  LightGBM: {weight_lgb:.4f} (ROC-AUC: {roc_auc_lgb:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Ensemble 예측\n",
    "y_val_pred_proba_weighted = weight_xgb * y_val_pred_proba_xgb + weight_lgb * y_val_pred_proba_lgb\n",
    "y_val_pred_weighted = (y_val_pred_proba_weighted >= 0.5).astype(int)\n",
    "\n",
    "y_test_pred_proba_weighted = weight_xgb * y_test_pred_proba_xgb + weight_lgb * y_test_pred_proba_lgb\n",
    "y_test_pred_weighted = (y_test_pred_proba_weighted >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Weighted Ensemble (Performance-based)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_weighted):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_weighted):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_weighted):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_weighted):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_weighted):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_weighted):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_weighted):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_weighted):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_weighted):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_weighted):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimized Weighted Ensemble\n",
    "\n",
    "`EnsembleModel.optimize_weights()` 메서드를 사용하여 최적 가중치 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnsembleModel 초기화 (equal weights)\n",
    "ensemble_model = EnsembleModel(models=[xgb_model, lgb_model], weights=[0.5, 0.5])\n",
    "\n",
    "print(\"Optimizing ensemble weights using Validation set...\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "# 가중치 최적화 (ROC-AUC 최대화)\n",
    "optimized_weights = ensemble_model.optimize_weights(\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    metric_func=roc_auc_score\n",
    ")\n",
    "\n",
    "print(f\"Optimized weights:\")\n",
    "print(f\"  XGBoost:  {optimized_weights[0]:.4f}\")\n",
    "print(f\"  LightGBM: {optimized_weights[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Ensemble 예측\n",
    "y_val_pred_proba_optimized = ensemble_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_optimized = ensemble_model.predict(X_val)\n",
    "\n",
    "y_test_pred_proba_optimized = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_optimized = ensemble_model.predict(X_test)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Optimized Weighted Ensemble\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_optimized):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_optimized):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_optimized):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_optimized):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_optimized):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_optimized):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_optimized):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_optimized):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_optimized):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_optimized):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Voting Ensemble\n",
    "\n",
    "Soft voting (확률 평균)과 Hard voting (다수결) 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting Ensemble\n",
    "voting_soft = VotingEnsemble(models=[xgb_model, lgb_model], voting='soft')\n",
    "\n",
    "y_val_pred_proba_soft = voting_soft.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_soft = voting_soft.predict(X_val)\n",
    "\n",
    "y_test_pred_proba_soft = voting_soft.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_soft = voting_soft.predict(X_test)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Soft Voting Ensemble\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_soft):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_soft):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_soft):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_soft):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_soft):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_soft):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_soft):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_soft):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_soft):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_soft):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Voting Ensemble\n",
    "voting_hard = VotingEnsemble(models=[xgb_model, lgb_model], voting='hard')\n",
    "\n",
    "y_val_pred_hard = voting_hard.predict(X_val)\n",
    "y_test_pred_hard = voting_hard.predict(X_test)\n",
    "\n",
    "# Hard voting은 predict_proba가 없으므로 soft voting의 proba 사용\n",
    "y_val_pred_proba_hard = voting_hard.predict_proba(X_val)[:, 1]\n",
    "y_test_pred_proba_hard = voting_hard.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Hard Voting Ensemble\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_hard):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_hard):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_hard):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_hard):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_hard):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_hard):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_hard):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_hard):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_hard):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_hard):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stacking Ensemble\n",
    "\n",
    "Base models의 예측을 feature로 사용하여 메타 모델(Logistic Regression) 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train set에 대한 Out-of-Fold 예측 생성 (5-fold)\nfrom sklearn.model_selection import StratifiedKFold\n\nprint(\"Generating out-of-fold predictions for stacking...\")\nprint(\"This may take a few minutes...\\n\")\n\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n\n# Train set OOF predictions\ntrain_meta_features = np.zeros((len(X_train), 2))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print(f\"Fold {fold}/{n_folds}...\")\n    \n    X_fold_train = X_train.iloc[train_idx]\n    y_fold_train = y_train.iloc[train_idx]\n    X_fold_val = X_train.iloc[val_idx]\n    \n    # XGBoost\n    fold_xgb = xgb.XGBClassifier(**xgb_model.get_params())\n    fold_xgb.fit(X_fold_train, y_fold_train, verbose=False)\n    train_meta_features[val_idx, 0] = fold_xgb.predict_proba(X_fold_val)[:, 1]\n    \n    # LightGBM with suppressed output\n    lgb_params = lgb_model.get_params()\n    lgb_params['verbose'] = -1  # Suppress output\n    fold_lgb = lgb.LGBMClassifier(**lgb_params)\n    fold_lgb.fit(X_fold_train, y_fold_train)\n    train_meta_features[val_idx, 1] = fold_lgb.predict_proba(X_fold_val)[:, 1]\n\nprint(\"\\nOOF predictions completed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation/Test set meta features\n",
    "val_meta_features = np.column_stack([\n",
    "    y_val_pred_proba_xgb,\n",
    "    y_val_pred_proba_lgb\n",
    "])\n",
    "\n",
    "test_meta_features = np.column_stack([\n",
    "    y_test_pred_proba_xgb,\n",
    "    y_test_pred_proba_lgb\n",
    "])\n",
    "\n",
    "print(f\"Train meta features shape: {train_meta_features.shape}\")\n",
    "print(f\"Val meta features shape: {val_meta_features.shape}\")\n",
    "print(f\"Test meta features shape: {test_meta_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta model 학습 (Logistic Regression)\n",
    "print(\"Training meta model (Logistic Regression)...\\n\")\n",
    "\n",
    "meta_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "meta_model.fit(train_meta_features, y_train)\n",
    "\n",
    "print(\"Meta model coefficients:\")\n",
    "print(f\"  XGBoost weight:  {meta_model.coef_[0][0]:.4f}\")\n",
    "print(f\"  LightGBM weight: {meta_model.coef_[0][1]:.4f}\")\n",
    "print(f\"  Intercept: {meta_model.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Ensemble 예측\n",
    "y_val_pred_proba_stack = meta_model.predict_proba(val_meta_features)[:, 1]\n",
    "y_val_pred_stack = meta_model.predict(val_meta_features)\n",
    "\n",
    "y_test_pred_proba_stack = meta_model.predict_proba(test_meta_features)[:, 1]\n",
    "y_test_pred_stack = meta_model.predict(test_meta_features)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Stacking Ensemble (Logistic Regression)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Precision: {precision_score(y_val, y_val_pred_stack):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_val, y_val_pred_stack):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_val, y_val_pred_stack):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_val, y_val_pred_proba_stack):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_val, y_val_pred_proba_stack):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_test_pred_stack):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_test_pred_stack):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_test_pred_stack):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_test_pred_proba_stack):.4f}\")\n",
    "print(f\"  PR-AUC:    {average_precision_score(y_test, y_test_pred_proba_stack):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델 성능 비교 테이블\n",
    "comparison_results = []\n",
    "\n",
    "models_info = [\n",
    "    ('XGBoost (Tuned)', y_val_pred_xgb, y_val_pred_proba_xgb, y_test_pred_xgb, y_test_pred_proba_xgb),\n",
    "    ('LightGBM (Baseline)', y_val_pred_lgb, y_val_pred_proba_lgb, y_test_pred_lgb, y_test_pred_proba_lgb),\n",
    "    ('Simple Averaging', y_val_pred_avg, y_val_pred_proba_avg, y_test_pred_avg, y_test_pred_proba_avg),\n",
    "    ('Weighted (Perf-based)', y_val_pred_weighted, y_val_pred_proba_weighted, y_test_pred_weighted, y_test_pred_proba_weighted),\n",
    "    ('Optimized Weighted', y_val_pred_optimized, y_val_pred_proba_optimized, y_test_pred_optimized, y_test_pred_proba_optimized),\n",
    "    ('Soft Voting', y_val_pred_soft, y_val_pred_proba_soft, y_test_pred_soft, y_test_pred_proba_soft),\n",
    "    ('Hard Voting', y_val_pred_hard, y_val_pred_proba_hard, y_test_pred_hard, y_test_pred_proba_hard),\n",
    "    ('Stacking', y_val_pred_stack, y_val_pred_proba_stack, y_test_pred_stack, y_test_pred_proba_stack),\n",
    "]\n",
    "\n",
    "for name, val_pred, val_proba, test_pred, test_proba in models_info:\n",
    "    result = {\n",
    "        'Model': name,\n",
    "        'Val_Precision': precision_score(y_val, val_pred),\n",
    "        'Val_Recall': recall_score(y_val, val_pred),\n",
    "        'Val_F1': f1_score(y_val, val_pred),\n",
    "        'Val_ROC_AUC': roc_auc_score(y_val, val_proba),\n",
    "        'Val_PR_AUC': average_precision_score(y_val, val_proba),\n",
    "        'Test_Precision': precision_score(y_test, test_pred),\n",
    "        'Test_Recall': recall_score(y_test, test_pred),\n",
    "        'Test_F1': f1_score(y_test, test_pred),\n",
    "        'Test_ROC_AUC': roc_auc_score(y_test, test_proba),\n",
    "        'Test_PR_AUC': average_precision_score(y_test, test_proba),\n",
    "    }\n",
    "    comparison_results.append(result)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 성능이 좋은 모델 찾기 (Test ROC-AUC 기준)\n",
    "best_idx = comparison_df['Test_ROC_AUC'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
    "best_test_roc_auc = comparison_df.loc[best_idx, 'Test_ROC_AUC']\n",
    "best_test_f1 = comparison_df.loc[best_idx, 'Test_F1']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {best_model_name}\")\n",
    "print(f\"Test ROC-AUC: {best_test_roc_auc:.4f}\")\n",
    "print(f\"Test F1 Score: {best_test_f1:.4f}\")\n",
    "\n",
    "# Baseline (XGBoost Tuned) 대비 개선율\n",
    "baseline_roc_auc = comparison_df.loc[comparison_df['Model'] == 'XGBoost (Tuned)', 'Test_ROC_AUC'].values[0]\n",
    "improvement = ((best_test_roc_auc - baseline_roc_auc) / baseline_roc_auc) * 100\n",
    "\n",
    "print(f\"\\nImprovement over XGBoost (Tuned):\")\n",
    "print(f\"  Baseline ROC-AUC: {baseline_roc_auc:.4f}\")\n",
    "print(f\"  Best ROC-AUC:     {best_test_roc_auc:.4f}\")\n",
    "print(f\"  Improvement:      {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 성능 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves 비교 (Test Set)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(models_info)))\n",
    "\n",
    "for i, (name, _, _, _, test_proba) in enumerate(models_info):\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})', linewidth=2, color=colors[i])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves Comparison - Test Set', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves 비교 (Test Set)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for i, (name, _, _, _, test_proba) in enumerate(models_info):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, test_proba)\n",
    "    pr_auc = average_precision_score(y_test, test_proba)\n",
    "    ax.plot(recall, precision, label=f'{name} (AUC = {pr_auc:.4f})', linewidth=2, color=colors[i])\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curves Comparison - Test Set', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메트릭별 성능 비교 Bar Chart (Test Set)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC']\n",
    "metric_names = ['Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    sorted_df = comparison_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    bars = ax.barh(range(len(sorted_df)), sorted_df[metric].values)\n",
    "    \n",
    "    # 최고 성능 모델 강조\n",
    "    best_idx_metric = sorted_df[metric].idxmax()\n",
    "    bars[list(sorted_df.index).index(best_idx_metric)].set_color('red')\n",
    "    \n",
    "    ax.set_yticks(range(len(sorted_df)))\n",
    "    ax.set_yticklabels(sorted_df['Model'].values)\n",
    "    ax.set_xlabel(metric_name, fontsize=11)\n",
    "    ax.set_title(f'{metric_name} Comparison (Test Set)', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, v in enumerate(sorted_df[metric].values):\n",
    "        ax.text(v, i, f' {v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 앙상블 분석\n",
    "\n",
    "### 11.1 가중치 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 앙상블 방법의 가중치 비교\n",
    "weights_comparison = pd.DataFrame({\n",
    "    'Method': ['Simple Avg', 'Perf-based', 'Optimized', 'Stacking (coef)'],\n",
    "    'XGBoost_Weight': [\n",
    "        0.5,\n",
    "        weight_xgb,\n",
    "        optimized_weights[0],\n",
    "        meta_model.coef_[0][0]\n",
    "    ],\n",
    "    'LightGBM_Weight': [\n",
    "        0.5,\n",
    "        weight_lgb,\n",
    "        optimized_weights[1],\n",
    "        meta_model.coef_[0][1]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nWeights Comparison:\")\n",
    "display(weights_comparison.round(4))\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(weights_comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, weights_comparison['XGBoost_Weight'], width, label='XGBoost')\n",
    "ax.bar(x + width/2, weights_comparison['LightGBM_Weight'], width, label='LightGBM')\n",
    "\n",
    "ax.set_ylabel('Weight', fontsize=12)\n",
    "ax.set_title('Ensemble Weights Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(weights_comparison['Method'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 모델 간 예측 차이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost와 LightGBM의 예측 확률 차이 분석 (Test Set)\n",
    "pred_diff = np.abs(y_test_pred_proba_xgb - y_test_pred_proba_lgb)\n",
    "\n",
    "print(f\"Prediction Difference Statistics (Test Set):\")\n",
    "print(f\"  Mean:   {pred_diff.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(pred_diff):.4f}\")\n",
    "print(f\"  Std:    {pred_diff.std():.4f}\")\n",
    "print(f\"  Max:    {pred_diff.max():.4f}\")\n",
    "\n",
    "# 예측 차이가 큰 케이스\n",
    "high_diff_threshold = np.percentile(pred_diff, 90)\n",
    "high_diff_indices = np.where(pred_diff > high_diff_threshold)[0]\n",
    "\n",
    "print(f\"\\nHigh disagreement cases (>90th percentile, diff > {high_diff_threshold:.4f}):\")\n",
    "print(f\"  Count: {len(high_diff_indices)}\")\n",
    "print(f\"  True positive rate: {y_test.iloc[high_diff_indices].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 차이 분포 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(pred_diff, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(pred_diff.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {pred_diff.mean():.4f}')\n",
    "axes[0].axvline(high_diff_threshold, color='orange', linestyle='--', linewidth=2, label=f'90th percentile: {high_diff_threshold:.4f}')\n",
    "axes[0].set_xlabel('Absolute Prediction Difference', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Prediction Differences (XGBoost vs LightGBM)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "scatter = axes[1].scatter(y_test_pred_proba_xgb, y_test_pred_proba_lgb, \n",
    "                          c=y_test, alpha=0.5, cmap='RdYlGn_r', s=20)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Perfect agreement')\n",
    "axes[1].set_xlabel('XGBoost Predicted Probability', fontsize=12)\n",
    "axes[1].set_ylabel('LightGBM Predicted Probability', fontsize=12)\n",
    "axes[1].set_title('XGBoost vs LightGBM Predictions', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1], label='True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Hard Cases 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 모델이 틀리는 케이스 (False Negatives)\n",
    "# True positive인데 모든 모델이 negative로 예측\n",
    "\n",
    "false_negatives_xgb = (y_test == 1) & (y_test_pred_xgb == 0)\n",
    "false_negatives_lgb = (y_test == 1) & (y_test_pred_lgb == 0)\n",
    "false_negatives_ensemble = (y_test == 1) & (y_test_pred_optimized == 0)\n",
    "\n",
    "# 모든 모델이 놓친 케이스\n",
    "hard_fn = false_negatives_xgb & false_negatives_lgb & false_negatives_ensemble\n",
    "\n",
    "print(f\"Hard Cases Analysis (Test Set):\")\n",
    "print(f\"\\nFalse Negatives:\")\n",
    "print(f\"  XGBoost:  {false_negatives_xgb.sum()} ({false_negatives_xgb.sum()/y_test.sum()*100:.1f}% of positives)\")\n",
    "print(f\"  LightGBM: {false_negatives_lgb.sum()} ({false_negatives_lgb.sum()/y_test.sum()*100:.1f}% of positives)\")\n",
    "print(f\"  Ensemble: {false_negatives_ensemble.sum()} ({false_negatives_ensemble.sum()/y_test.sum()*100:.1f}% of positives)\")\n",
    "print(f\"\\nHard cases (all models missed): {hard_fn.sum()} ({hard_fn.sum()/y_test.sum()*100:.1f}% of positives)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives 분석\n",
    "false_positives_xgb = (y_test == 0) & (y_test_pred_xgb == 1)\n",
    "false_positives_lgb = (y_test == 0) & (y_test_pred_lgb == 1)\n",
    "false_positives_ensemble = (y_test == 0) & (y_test_pred_optimized == 1)\n",
    "\n",
    "# 모든 모델이 잘못 예측한 케이스\n",
    "hard_fp = false_positives_xgb & false_positives_lgb & false_positives_ensemble\n",
    "\n",
    "print(f\"\\nFalse Positives:\")\n",
    "print(f\"  XGBoost:  {false_positives_xgb.sum()} ({false_positives_xgb.sum()/(y_test==0).sum()*100:.1f}% of negatives)\")\n",
    "print(f\"  LightGBM: {false_positives_lgb.sum()} ({false_positives_lgb.sum()/(y_test==0).sum()*100:.1f}% of negatives)\")\n",
    "print(f\"  Ensemble: {false_positives_ensemble.sum()} ({false_positives_ensemble.sum()/(y_test==0).sum()*100:.1f}% of negatives)\")\n",
    "print(f\"\\nHard cases (all models wrong): {hard_fp.sum()} ({hard_fp.sum()/(y_test==0).sum()*100:.1f}% of negatives)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard cases의 예측 확률 분포\n",
    "if hard_fn.sum() > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # False Negatives (hard cases)\n",
    "    axes[0].hist(y_test_pred_proba_xgb[hard_fn], bins=20, alpha=0.5, label='XGBoost', edgecolor='black')\n",
    "    axes[0].hist(y_test_pred_proba_lgb[hard_fn], bins=20, alpha=0.5, label='LightGBM', edgecolor='black')\n",
    "    axes[0].hist(y_test_pred_proba_optimized[hard_fn], bins=20, alpha=0.5, label='Ensemble', edgecolor='black')\n",
    "    axes[0].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "    axes[0].set_xlabel('Predicted Probability', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title(f'Predicted Probabilities for Hard False Negatives (n={hard_fn.sum()})', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # False Positives (hard cases)\n",
    "    if hard_fp.sum() > 0:\n",
    "        axes[1].hist(y_test_pred_proba_xgb[hard_fp], bins=20, alpha=0.5, label='XGBoost', edgecolor='black')\n",
    "        axes[1].hist(y_test_pred_proba_lgb[hard_fp], bins=20, alpha=0.5, label='LightGBM', edgecolor='black')\n",
    "        axes[1].hist(y_test_pred_proba_optimized[hard_fp], bins=20, alpha=0.5, label='Ensemble', edgecolor='black')\n",
    "        axes[1].axvline(0.5, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "        axes[1].set_xlabel('Predicted Probability', fontsize=12)\n",
    "        axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "        axes[1].set_title(f'Predicted Probabilities for Hard False Positives (n={hard_fp.sum()})', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No hard false negative cases found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 최종 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델 저장 (Optimized Weighted Ensemble)\n",
    "print(\"Saving ensemble models...\\n\")\n",
    "\n",
    "# 1. Best ensemble model (Optimized Weighted)\n",
    "with open(model_dir / 'ensemble_best.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_model, f)\n",
    "print(f\"Saved: {model_dir / 'ensemble_best.pkl'}\")\n",
    "\n",
    "# 2. Simple averaging (backup)\n",
    "simple_ensemble = EnsembleModel(models=[xgb_model, lgb_model], weights=[0.5, 0.5])\n",
    "with open(model_dir / 'ensemble_simple_avg.pkl', 'wb') as f:\n",
    "    pickle.dump(simple_ensemble, f)\n",
    "print(f\"Saved: {model_dir / 'ensemble_simple_avg.pkl'}\")\n",
    "\n",
    "# 3. Stacking model\n",
    "stacking_model = {\n",
    "    'base_models': [xgb_model, lgb_model],\n",
    "    'meta_model': meta_model\n",
    "}\n",
    "with open(model_dir / 'ensemble_stacking.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_model, f)\n",
    "print(f\"Saved: {model_dir / 'ensemble_stacking.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "ensemble_results = {\n",
    "    'best_model': best_model_name,\n",
    "    'test_metrics': {\n",
    "        'roc_auc': float(best_test_roc_auc),\n",
    "        'f1_score': float(best_test_f1),\n",
    "        'precision': float(comparison_df.loc[best_idx, 'Test_Precision']),\n",
    "        'recall': float(comparison_df.loc[best_idx, 'Test_Recall']),\n",
    "        'pr_auc': float(comparison_df.loc[best_idx, 'Test_PR_AUC']),\n",
    "    },\n",
    "    'optimized_weights': {\n",
    "        'xgboost': float(optimized_weights[0]),\n",
    "        'lightgbm': float(optimized_weights[1])\n",
    "    },\n",
    "    'stacking_coefficients': {\n",
    "        'xgboost': float(meta_model.coef_[0][0]),\n",
    "        'lightgbm': float(meta_model.coef_[0][1]),\n",
    "        'intercept': float(meta_model.intercept_[0])\n",
    "    },\n",
    "    'improvement_over_baseline': {\n",
    "        'baseline_roc_auc': float(baseline_roc_auc),\n",
    "        'improvement_percent': float(improvement)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(model_dir / 'ensemble_results.json', 'w') as f:\n",
    "    json.dump(ensemble_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved: {model_dir / 'ensemble_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교 테이블 CSV 저장\n",
    "comparison_df.to_csv(model_dir / 'ensemble_comparison.csv', index=False)\n",
    "print(f\"Saved: {model_dir / 'ensemble_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 결론\n",
    "\n",
    "### 완료된 작업\n",
    "1. ✅ XGBoost와 LightGBM 기존 모델 로드\n",
    "2. ✅ Simple Averaging Ensemble 구현\n",
    "3. ✅ Weighted Ensemble (성능 기반) 구현\n",
    "4. ✅ Optimized Weighted Ensemble 구현\n",
    "5. ✅ Voting Ensemble (Soft/Hard) 구현\n",
    "6. ✅ Stacking Ensemble 구현\n",
    "7. ✅ 모델 성능 비교 및 시각화\n",
    "8. ✅ 앙상블 분석 (가중치, 예측 차이, Hard cases)\n",
    "9. ✅ 최종 모델 저장\n",
    "\n",
    "### 주요 결과\n",
    "- 총 8개 모델 비교 (단일 모델 2개 + 앙상블 6개)\n",
    "- 앙상블 모델이 단일 모델보다 일반적으로 더 나은 성능\n",
    "- Optimized Weighted Ensemble이 자동으로 최적 가중치 탐색\n",
    "- Stacking은 메타 학습을 통해 모델 간 상호작용 학습\n",
    "\n",
    "### 다음 단계\n",
    "1. **임계값 최적화**: Business metric 기반 threshold 조정\n",
    "2. **모델 해석**: SHAP을 앙상블 모델에 적용\n",
    "3. **프로덕션 배포**: 모델 서빙 파이프라인 구축\n",
    "4. **모니터링**: 성능 모니터링 및 재학습 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE MODEL TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test ROC-AUC: {best_test_roc_auc:.4f}\")\n",
    "print(f\"Test F1 Score: {best_test_f1:.4f}\")\n",
    "print(f\"Improvement over baseline: {improvement:+.2f}%\")\n",
    "print(f\"\\nModels saved to: {model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}