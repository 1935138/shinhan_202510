# XGBoost/LightGBM Í∏∞Î∞ò Í∞ÄÎßπÏ†ê ÏúÑÍ∏∞ Ï°∞Í∏∞ Í≤ΩÎ≥¥ ÏãúÏä§ÌÖú Íµ¨ÌòÑ Í≥ÑÌöç

## üìã Î™©Ï∞®
1. [ÌîÑÎ°úÏ†ùÌä∏ Í∞úÏöî](#1-ÌîÑÎ°úÏ†ùÌä∏-Í∞úÏöî)
2. [Íµ¨ÌòÑ ÏïÑÌÇ§ÌÖçÏ≤ò](#2-Íµ¨ÌòÑ-ÏïÑÌÇ§ÌÖçÏ≤ò)
3. [Îã®Í≥ÑÎ≥Ñ Íµ¨ÌòÑ Í≥ÑÌöç](#3-Îã®Í≥ÑÎ≥Ñ-Íµ¨ÌòÑ-Í≥ÑÌöç)
4. [Í∏∞Ïà† Ïä§ÌÉù](#4-Í∏∞Ïà†-Ïä§ÌÉù)
5. [Íµ¨ÌòÑ ÏÑ∏Î∂ÄÏÇ¨Ìï≠](#5-Íµ¨ÌòÑ-ÏÑ∏Î∂ÄÏÇ¨Ìï≠)
6. [ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞](#6-ÎîîÎ†âÌÜ†Î¶¨-Íµ¨Ï°∞)
7. [ÏòàÏÉÅ Í≤∞Í≥ºÎ¨º](#7-ÏòàÏÉÅ-Í≤∞Í≥ºÎ¨º)

---

## 1. ÌîÑÎ°úÏ†ùÌä∏ Í∞úÏöî

### 1.1 Î™©Ìëú
**ÏòÅÏÑ∏/Ï§ëÏÜå ÏöîÏãù Í∞ÄÎßπÏ†êÏùò Í≤ΩÏòÅ ÏúÑÍ∏∞Î•º 1~3Í∞úÏõî Ï†ÑÏóê ÏòàÏ∏°ÌïòÎäî AI Ï°∞Í∏∞ Í≤ΩÎ≥¥ ÏãúÏä§ÌÖú Í∞úÎ∞ú**

### 1.2 ÌïµÏã¨ Í≥ºÏ†ú
- ÌèêÏóÖ ÏúÑÌóò Í∞ÄÎßπÏ†ê Ï°∞Í∏∞ ÏãùÎ≥Ñ (Recall ÏµúÎåÄÌôî)
- ÏúÑÍ∏∞ Ïã†Ìò∏ Ìï¥ÏÑù Î∞è Ïù∏ÏÇ¨Ïù¥Ìä∏ ÎèÑÏ∂ú (SHAP)
- ÎßûÏ∂§Ìòï Í∏àÏúµÏÉÅÌíà/ÏÑúÎπÑÏä§ Ï†úÏïà

### 1.3 Ïôú XGBoost/LightGBMÏù∏Í∞Ä?

#### ÏÑ†ÌÉù Ïù¥Ïú†
1. **Î∂àÍ∑†Ìòï Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Ïö∞Ïàò**: ÌèêÏóÖÎ•† 3% vs Ïö¥ÏòÅÏ§ë 97%
2. **Ìï¥ÏÑù Í∞ÄÎä•ÏÑ±**: Feature Importance + SHAP values
3. **Í≤∞Ï∏°Í∞í ÏûêÎèô Ï≤òÎ¶¨**: SV(-999999.9) Í∞í Ìè¨Ìï®
4. **Î≤îÏ£ºÌòï Îç∞Ïù¥ÌÑ∞ Ìö®Í≥ºÏ†Å Ï≤òÎ¶¨**: 6Îã®Í≥Ñ Íµ¨Í∞Ñ Îç∞Ïù¥ÌÑ∞
5. **Îπ†Î•∏ ÌïôÏäµ ÏÜçÎèÑ**: Îã§ÏñëÌïú Ïã§Ìóò Í∞ÄÎä•
6. **Í≤ÄÏ¶ùÎêú ÏÑ±Îä•**: Kaggle Îì± ÎåÄÌöå ÏÉÅÏúÑÍ∂å ÏÜîÎ£®ÏÖò

#### Ïû•Ï†ê ÎπÑÍµê

| ÌäπÏÑ± | XGBoost | LightGBM |
|------|---------|----------|
| **ÌïôÏäµ ÏÜçÎèÑ** | Î≥¥ÌÜµ | Îπ†Î¶Ñ ‚ö° |
| **Î©îÎ™®Î¶¨ Ìö®Ïú®** | Î≥¥ÌÜµ | Ïö∞Ïàò |
| **ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞** | Ï†úÌïúÏ†Å | Ïö∞Ïàò |
| **Î∂àÍ∑†Ìòï Ï≤òÎ¶¨** | `scale_pos_weight` | `is_unbalance=True` |
| **Î≤îÏ£ºÌòï Ï≤òÎ¶¨** | Label encoding ÌïÑÏöî | ÏûêÎèô ÏßÄÏõê |
| **Í≥ºÏ†ÅÌï© Î∞©ÏßÄ** | Ïö∞Ïàò | Ïö∞Ïàò |

**‚Üí Îëê Î™®Îç∏ Î™®Îëê ÌïôÏäµ ÌõÑ ÏïôÏÉÅÎ∏îÎ°ú ÏµúÏ¢Ö ÏÑ±Îä• Í∑πÎåÄÌôî**

---

## 2. Íµ¨ÌòÑ ÏïÑÌÇ§ÌÖçÏ≤ò

### 2.1 Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏

```
[ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞]
    ‚Üì
[Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨]
- ÎÇ†Ïßú Î≥ÄÌôò
- SV Ï≤òÎ¶¨
- Íµ¨Í∞Ñ Ïù∏ÏΩîÎî©
    ‚Üì
[Feature Engineering]
- ÏãúÍ≥ÑÏó¥ ÌäπÏÑ± (lag, rolling)
- ÏÉÅÎåÄÏ†Å ÏßÄÌëú (ÏàúÏúÑ Î≥ÄÌôî)
- Í≥†Í∞ù ÌñâÎèô (Ïû¨Î∞©Î¨∏Ïú® Ï∂îÏÑ∏)
- Î≥µÌï© ÏßÄÌëú (Îß§Ï∂ú-Í≥†Í∞ù Í¥¥Î¶¨)
    ‚Üì
[Train/Valid/Test Split]
- ÏãúÍ∞Ñ Í∏∞Î∞ò Î∂ÑÌï†
- 202401-202406: Train
- 202407-202409: Valid
- 202410-202412: Test
    ‚Üì
[Î∂àÍ∑†Ìòï Ï≤òÎ¶¨]
- SMOTE (Over-sampling)
- Class Weight Ï°∞Ï†ï
    ‚Üì
[Î™®Îç∏ ÌïôÏäµ]
- XGBoost
- LightGBM
- (CatBoost - ÏòµÏÖò)
    ‚Üì
[ÏïôÏÉÅÎ∏î]
- Voting (Soft)
- Stacking
    ‚Üì
[Î™®Îç∏ Ìï¥ÏÑù]
- SHAP values
- Feature Importance
- Partial Dependence Plot
    ‚Üì
[ÏúÑÍ∏∞ Ïã†Ìò∏ Î∂ÑÏÑù]
- Ï£ºÏöî ÏúÑÌóò ÏöîÏù∏ Top 10
- ÏóÖÏ¢Ö/ÏÉÅÍ∂åÎ≥Ñ Ìå®ÌÑ¥
- ÏúÑÌóò Ïú†Ìòï Î∂ÑÎ•ò
    ‚Üì
[ÎπÑÏ¶àÎãàÏä§ Ï†úÏïà]
- Í∏àÏúµÏÉÅÌíà Îß§Ïπ≠
- Í∞úÏûÖ Ï†ÑÎûµ
```

### 2.2 Î™®Îç∏ Íµ¨Ï°∞

#### Í∞úÎ≥Ñ Î™®Îç∏
```python
# XGBoost Î™®Îç∏
xgb_model = XGBClassifier(
    objective='binary:logistic',
    scale_pos_weight=32,  # (4058/127)
    max_depth=6,
    learning_rate=0.05,
    n_estimators=1000,
    subsample=0.8,
    colsample_bytree=0.8,
    early_stopping_rounds=50
)

# LightGBM Î™®Îç∏
lgb_model = LGBMClassifier(
    objective='binary',
    is_unbalance=True,
    max_depth=6,
    learning_rate=0.05,
    n_estimators=1000,
    subsample=0.8,
    colsample_bytree=0.8,
    early_stopping_rounds=50
)
```

#### ÏïôÏÉÅÎ∏î Ï†ÑÎûµ
1. **Soft Voting**: ÌôïÎ•† ÌèâÍ∑†
2. **Stacking**: Logistic Regression meta-learner
3. **Í∞ÄÏ§ë ÌèâÍ∑†**: Í≤ÄÏ¶ù ÏÑ±Îä• Í∏∞Î∞ò Í∞ÄÏ§ëÏπò

---

## 3. Îã®Í≥ÑÎ≥Ñ Íµ¨ÌòÑ Í≥ÑÌöç

### Week 1: Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Î∞è Í∏∞Î≥∏ Feature (11/18 ~ 11/24)

#### Î™©Ìëú
- 3Í∞ú Îç∞Ïù¥ÌÑ∞ÏÖã ÌÜµÌï©
- Í≤∞Ï∏°Í∞í(SV) Ï≤òÎ¶¨
- Í∏∞Î≥∏ ÌÉÄÍ≤ü Î≥ÄÏàò ÏÉùÏÑ±

#### ÏûëÏóÖ ÎÇ¥Ïó≠
1. **Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Î≥ëÌï©**
   ```python
   # Dataset 1 + 2 + 3 ÌÜµÌï©
   # Step 1: Dataset 2(ÏõîÎ≥Ñ Îß§Ï∂ú/Ïù¥Ïö© ÌòÑÌô©) + Dataset 3(ÏõîÎ≥Ñ Í≥†Í∞ù Ï†ïÎ≥¥)Î•º Í∞ÄÎßπÏ†êIDÏôÄ ÎÖÑÏõî Í∏∞Ï§ÄÏúºÎ°ú Î≥ëÌï©
   #         ‚Üí Í∞ôÏùÄ Í∞ÄÎßπÏ†êÏùò Í∞ôÏùÄ Ïõî Îç∞Ïù¥ÌÑ∞Î•º ÏòÜÏúºÎ°ú Î∂ôÏûÑ
   df_merged = df2.merge(df3, on=['ENCODED_MCT', 'TA_YM'])

   # Step 2: ÏúÑ Í≤∞Í≥º + Dataset 1(Í∞ÄÎßπÏ†ê Í∏∞Î≥∏Ï†ïÎ≥¥: ÏóÖÏ¢Ö, ÏúÑÏπò, ÌèêÏóÖÏùº)ÏùÑ Í∞ÄÎßπÏ†êID Í∏∞Ï§ÄÏúºÎ°ú Î≥ëÌï©
   #         ‚Üí Í∞ÄÎßπÏ†êÎ≥Ñ Í≥†Ï†ï Ï†ïÎ≥¥Î•º Î™®Îì† Ïõî Îç∞Ïù¥ÌÑ∞Ïóê Ï∂îÍ∞Ä
   df_full = df_merged.merge(df1[['ENCODED_MCT', ...]], on='ENCODED_MCT')

   # ÏµúÏ¢Ö: Í∞Å ÌñâÏù¥ "ÌäπÏ†ï Í∞ÄÎßπÏ†êÏùò ÌäπÏ†ï Ïõî" Îç∞Ïù¥ÌÑ∞Í∞Ä Îê® (ÏãúÍ≥ÑÏó¥ Î∂ÑÏÑù Í∞ÄÎä•)
   ```

2. **ÌÉÄÍ≤ü Î≥ÄÏàò ÏÉùÏÑ±**
   ```python
   # ÌèêÏóÖ Ïó¨Î∂Ä
   df['is_closed'] = df['MCT_ME_D'].notna().astype(int)

   # ÎØ∏Îûò NÍ∞úÏõî ÎÇ¥ ÌèêÏóÖ (Ï°∞Í∏∞ Í≤ΩÎ≥¥Ïö©)
   df['will_close_1m'] = ...
   df['will_close_3m'] = ...
   ```

3. **Í≤∞Ï∏°Í∞í Ï≤òÎ¶¨**
   ```python
   # SV(-999999.9) ‚Üí NaN Î≥ÄÌôò
   df.replace(-999999.9, np.nan, inplace=True)

   # Ï†ÑÎûµ:
   # - Î∞∞Îã¨Îß§Ï∂ú: 0ÏúºÎ°ú ÎåÄÏ≤¥ (Î∞∞Îã¨ ÎØ∏Ïö¥ÏòÅ)
   # - Í≥†Í∞ùÏ†ïÎ≥¥: Ï§ëÏïôÍ∞í ÎòêÎäî Î≥ÑÎèÑ ÌîåÎûòÍ∑∏ ÏÉùÏÑ±
   ```

4. **Íµ¨Í∞Ñ Îç∞Ïù¥ÌÑ∞ Ïù∏ÏΩîÎî©**
   ```python
   interval_mapping = {
       '1_10%Ïù¥Ìïò': 1,
       '2_10-25%': 2,
       '3_25-50%': 3,
       '4_50-75%': 4,
       '5_75-90%': 5,
       '6_90%Ï¥àÍ≥º(ÌïòÏúÑ 10% Ïù¥Ìïò)': 6
   }
   ```

#### ÏÇ∞Ï∂úÎ¨º
- `notebooks/02_preprocessing.ipynb`
- `pipeline/preprocessing/data_loader.py`
- `pipeline/preprocessing/feature_encoder.py`

---

### Week 2: ÏãúÍ≥ÑÏó¥ Feature Engineering (11/25 ~ 12/01)

#### Î™©Ìëú
- ÏãúÍ≥ÑÏó¥ ÌäπÏÑ± ÏÉùÏÑ± (100+ features)
- Feature selection

#### ÏûëÏóÖ ÎÇ¥Ïó≠

1. **ÏãúÍ≥ÑÏó¥ Í∏∞Î≥∏ ÌäπÏÑ±**
   ```python
   # Lag features
   for col in ['RC_M1_SAA', 'RC_M1_TO_UE_CT', 'RC_M1_UE_CUS_CN']:
       for lag in [1, 3, 6, 12]:
           df[f'{col}_lag_{lag}'] = df.groupby('ENCODED_MCT')[col].shift(lag)

   # Ïù¥ÎèôÌèâÍ∑†
   for window in [3, 6]:
       df[f'sales_ma_{window}m'] = df.groupby('ENCODED_MCT')['sales'].transform(
           lambda x: x.rolling(window, min_periods=1).mean()
       )

   # Ï¶ùÍ∞êÎ•†
   df['sales_mom'] = df.groupby('ENCODED_MCT')['sales'].pct_change()
   df['sales_qoq'] = df.groupby('ENCODED_MCT')['sales'].pct_change(3)
   ```

2. **Ï∂îÏÑ∏ Î∞è Î≥ÄÎèôÏÑ±**
   ```python
   # Ïó∞ÏÜç ÌïòÎùΩ Í∞úÏõî Ïàò
   df['consecutive_decline'] = ...

   # Î≥ÄÎèô Í≥ÑÏàò
   df['cv_sales_3m'] = (
       df.groupby('ENCODED_MCT')['sales'].transform(lambda x: x.rolling(3).std()) /
       df.groupby('ENCODED_MCT')['sales'].transform(lambda x: x.rolling(3).mean())
   )
   ```

3. **ÏÉÅÎåÄÏ†Å ÏßÄÌëú**
   ```python
   # ÏóÖÏ¢Ö ÎÇ¥ ÏàúÏúÑ Î≥ÄÌôî
   df['rank_change_industry'] = df.groupby('ENCODED_MCT')['M12_SME_RY_SAA_PCE_RT'].diff()

   # ÏóÖÏ¢Ö ÌèâÍ∑† ÎåÄÎπÑ Í≤©Ï∞®
   df['gap_from_industry'] = df['M1_SME_RY_SAA_RAT'] - 100
   ```

4. **Í≥†Í∞ù ÌñâÎèô ÏßÄÌëú**
   ```python
   # Ïû¨Î∞©Î¨∏Ïú® Î≥ÄÌôî
   df['revisit_rate_change'] = df.groupby('ENCODED_MCT')['MCT_UE_CLN_REU_RAT'].diff()

   # Í≥†Í∞ù Îã§ÏñëÏÑ± (Entropy)
   from scipy.stats import entropy
   age_cols = ['M12_MAL_1020_RAT', 'M12_MAL_30_RAT', ...]
   df['customer_diversity'] = df[age_cols].apply(
       lambda row: entropy(row / row.sum()) if row.sum() > 0 else 0, axis=1
   )
   ```

5. **Î≥µÌï© ÏßÄÌëú**
   ```python
   # Îß§Ï∂ú-Í≥†Í∞ù Í¥¥Î¶¨ÎèÑ
   df['sales_customer_gap'] = (
       df['sales_change'] - df['customer_change']
   )

   # Î∞∞Îã¨ ÏùòÏ°¥ÎèÑ Ï¶ùÍ∞Ä + Îß§Ï∂ú Í∞êÏÜå
   df['delivery_risk'] = (
       (df['delivery_ratio_change'] > 10) & (df['sales_change'] < 0)
   ).astype(int)
   ```

6. **Feature Selection**
   ```python
   from sklearn.feature_selection import mutual_info_classif

   # ÏÉÅÌò∏Ï†ïÎ≥¥Îüâ Í∏∞Î∞ò ÏÑ†ÌÉù
   mi_scores = mutual_info_classif(X, y, random_state=42)
   top_features = mi_scores.argsort()[-100:]  # ÏÉÅÏúÑ 100Í∞ú
   ```

#### ÏÇ∞Ï∂úÎ¨º
- `notebooks/03_feature_engineering.ipynb`
- `pipeline/features/time_series_features.py`
- `pipeline/features/customer_features.py`
- `pipeline/features/composite_features.py`

---

### Week 3: Î™®Îç∏ Í∞úÎ∞ú (XGBoost/LightGBM) (12/02 ~ 12/08)

#### Î™©Ìëú
- XGBoost/LightGBM Î≤†Ïù¥Ïä§ÎùºÏù∏ Íµ¨Ï∂ï
- Î∂àÍ∑†Ìòï Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
- Ï¥àÍ∏∞ ÏÑ±Îä• ÌèâÍ∞Ä

#### ÏûëÏóÖ ÎÇ¥Ïó≠

1. **Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†**
   ```python
   # ÏãúÍ∞Ñ Í∏∞Î∞ò Î∂ÑÌï†
   train = df[df['TA_YM'] <= '202406']
   valid = df[(df['TA_YM'] > '202406') & (df['TA_YM'] <= '202409')]
   test = df[df['TA_YM'] > '202409']

   # ÌäπÏÑ±/ÌÉÄÍ≤ü Î∂ÑÎ¶¨
   feature_cols = [col for col in train.columns if col not in
                   ['ENCODED_MCT', 'TA_YM', 'is_closed', 'MCT_ME_D', ...]]
   X_train, y_train = train[feature_cols], train['is_closed']
   ```

2. **Î∂àÍ∑†Ìòï Ï≤òÎ¶¨**
   ```python
   # SMOTE Over-sampling
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(sampling_strategy=0.3, random_state=42)
   X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

   # Class Weight Í≥ÑÏÇ∞
   scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
   ```

3. **XGBoost ÌïôÏäµ**
   ```python
   import xgboost as xgb

   xgb_model = xgb.XGBClassifier(
       objective='binary:logistic',
       eval_metric='aucpr',
       scale_pos_weight=scale_pos_weight,
       max_depth=6,
       learning_rate=0.05,
       n_estimators=1000,
       subsample=0.8,
       colsample_bytree=0.8,
       gamma=1,
       random_state=42
   )

   xgb_model.fit(
       X_train, y_train,
       eval_set=[(X_valid, y_valid)],
       early_stopping_rounds=50,
       verbose=50
   )
   ```

4. **LightGBM ÌïôÏäµ**
   ```python
   import lightgbm as lgb

   lgb_model = lgb.LGBMClassifier(
       objective='binary',
       metric='auc',
       is_unbalance=True,
       max_depth=6,
       learning_rate=0.05,
       n_estimators=1000,
       subsample=0.8,
       colsample_bytree=0.8,
       min_child_samples=20,
       random_state=42
   )

   lgb_model.fit(
       X_train, y_train,
       eval_set=[(X_valid, y_valid)],
       callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]
   )
   ```

5. **ÌèâÍ∞Ä**
   ```python
   from sklearn.metrics import classification_report, precision_recall_curve, auc

   # ÏòàÏ∏°
   y_pred_xgb = xgb_model.predict(X_valid)
   y_proba_xgb = xgb_model.predict_proba(X_valid)[:, 1]

   # PR-AUC
   precision, recall, _ = precision_recall_curve(y_valid, y_proba_xgb)
   pr_auc = auc(recall, precision)

   # Classification Report
   print(classification_report(y_valid, y_pred_xgb))
   ```

#### ÏÇ∞Ï∂úÎ¨º
- `notebooks/04_model_xgboost.ipynb`
- `notebooks/05_model_lightgbm.ipynb`
- `pipeline/models/xgboost_model.py`
- `pipeline/models/lightgbm_model.py`
- `models/xgb_baseline.pkl`
- `models/lgb_baseline.pkl`

---

### Week 4: ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù Î∞è ÏïôÏÉÅÎ∏î (12/09 ~ 12/15)

#### Î™©Ìëú
- Grid Search / Optuna ÌäúÎãù
- ÏïôÏÉÅÎ∏î Î™®Îç∏ Íµ¨Ï∂ï
- ÏµúÏ¢Ö ÏÑ±Îä• ÏµúÏ†ÅÌôî

#### ÏûëÏóÖ ÎÇ¥Ïó≠

1. **ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù (Optuna)**
   ```python
   import optuna

   def objective(trial):
       params = {
           'max_depth': trial.suggest_int('max_depth', 3, 10),
           'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
           'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
           'subsample': trial.suggest_float('subsample', 0.6, 1.0),
           'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
           'gamma': trial.suggest_float('gamma', 0, 5),
           'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
           'scale_pos_weight': scale_pos_weight
       }

       model = xgb.XGBClassifier(**params, random_state=42)
       model.fit(X_train, y_train,
                 eval_set=[(X_valid, y_valid)],
                 early_stopping_rounds=50,
                 verbose=False)

       y_proba = model.predict_proba(X_valid)[:, 1]
       precision, recall, _ = precision_recall_curve(y_valid, y_proba)
       pr_auc = auc(recall, precision)

       return pr_auc

   study = optuna.create_study(direction='maximize')
   study.optimize(objective, n_trials=100)
   best_params = study.best_params
   ```

2. **ÏãúÍ≥ÑÏó¥ ÍµêÏ∞® Í≤ÄÏ¶ù**
   ```python
   from sklearn.model_selection import TimeSeriesSplit

   tscv = TimeSeriesSplit(n_splits=5, test_size=3)
   pr_aucs = []

   for train_idx, val_idx in tscv.split(X):
       X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
       y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]

       model.fit(X_tr, y_tr)
       y_proba = model.predict_proba(X_val)[:, 1]

       precision, recall, _ = precision_recall_curve(y_val, y_proba)
       pr_aucs.append(auc(recall, precision))

   print(f"Average PR-AUC: {np.mean(pr_aucs):.4f} ¬± {np.std(pr_aucs):.4f}")
   ```

3. **ÏïôÏÉÅÎ∏î - Voting Classifier**
   ```python
   from sklearn.ensemble import VotingClassifier

   voting_clf = VotingClassifier(
       estimators=[
           ('xgb', xgb_model),
           ('lgb', lgb_model)
       ],
       voting='soft',
       weights=[0.6, 0.4]  # Í≤ÄÏ¶ù ÏÑ±Îä• Í∏∞Î∞ò
   )

   voting_clf.fit(X_train, y_train)
   ```

4. **ÏïôÏÉÅÎ∏î - Stacking**
   ```python
   from sklearn.ensemble import StackingClassifier
   from sklearn.linear_model import LogisticRegression

   stacking_clf = StackingClassifier(
       estimators=[
           ('xgb', xgb_model),
           ('lgb', lgb_model)
       ],
       final_estimator=LogisticRegression(class_weight='balanced'),
       cv=TimeSeriesSplit(n_splits=5)
   )

   stacking_clf.fit(X_train, y_train)
   ```

5. **ÏûÑÍ≥ÑÍ∞í ÏµúÏ†ÅÌôî**
   ```python
   # Precision-Recall Ìä∏Î†àÏù¥ÎìúÏò§ÌîÑ
   precision, recall, thresholds = precision_recall_curve(y_valid, y_proba)

   # F2-score ÏµúÎåÄÌôî (Recall Ï§ëÏãú)
   from sklearn.metrics import fbeta_score
   f2_scores = [fbeta_score(y_valid, y_proba >= t, beta=2) for t in thresholds]
   optimal_threshold = thresholds[np.argmax(f2_scores)]

   # ÏµúÏ†Å ÏûÑÍ≥ÑÍ∞í Ï†ÅÏö©
   y_pred_optimal = (y_proba >= optimal_threshold).astype(int)
   ```

#### ÏÇ∞Ï∂úÎ¨º
- `notebooks/06_hyperparameter_tuning.ipynb`
- `notebooks/07_ensemble.ipynb`
- `pipeline/models/ensemble.py`
- `models/xgb_tuned.pkl`
- `models/lgb_tuned.pkl`
- `models/ensemble_voting.pkl`
- `models/ensemble_stacking.pkl`

---

### Week 5: Î™®Îç∏ Ìï¥ÏÑù (SHAP) Î∞è Ïù∏ÏÇ¨Ïù¥Ìä∏ ÎèÑÏ∂ú (12/16 ~ 12/22)

#### Î™©Ìëú
- SHAP values Î∂ÑÏÑù
- ÏúÑÍ∏∞ Ïã†Ìò∏ Top 10 ÎèÑÏ∂ú
- ÏóÖÏ¢Ö/ÏÉÅÍ∂åÎ≥Ñ Ìå®ÌÑ¥ Î∂ÑÏÑù

#### ÏûëÏóÖ ÎÇ¥Ïó≠

1. **SHAP Î∂ÑÏÑù - Ï†ÑÏó≠Ï†Å Ìï¥ÏÑù**
   ```python
   import shap

   # SHAP Explainer
   explainer = shap.TreeExplainer(xgb_model)
   shap_values = explainer.shap_values(X_test)

   # Summary Plot (Ï†ÑÏ≤¥ ÌäπÏÑ± Ï§ëÏöîÎèÑ)
   shap.summary_plot(shap_values, X_test, plot_type='bar')
   shap.summary_plot(shap_values, X_test)

   # Feature Importance
   feature_importance = pd.DataFrame({
       'feature': X_test.columns,
       'shap_importance': np.abs(shap_values).mean(axis=0)
   }).sort_values('shap_importance', ascending=False)

   print("ÏúÑÍ∏∞ Ïã†Ìò∏ Top 10:")
   print(feature_importance.head(10))
   ```

2. **SHAP Î∂ÑÏÑù - Í∞úÎ≥Ñ Í∞ÄÎßπÏ†ê Ìï¥ÏÑù**
   ```python
   # ÌèêÏóÖ ÏòàÏ∏°Îêú Í∞ÄÎßπÏ†ê ÏÇ¨Î°Ä
   high_risk_idx = np.where(y_proba_test > 0.8)[0][0]

   # Waterfall Plot
   shap.waterfall_plot(shap.Explanation(
       values=shap_values[high_risk_idx],
       base_values=explainer.expected_value,
       data=X_test.iloc[high_risk_idx],
       feature_names=X_test.columns.tolist()
   ))

   # Force Plot
   shap.force_plot(
       explainer.expected_value,
       shap_values[high_risk_idx],
       X_test.iloc[high_risk_idx]
   )
   ```

3. **Dependence Plot (Î≥ÄÏàò Í∞Ñ ÏÉÅÌò∏ÏûëÏö©)**
   ```python
   # Îß§Ï∂ú ÎåÄÎπÑ Ïû¨Î∞©Î¨∏Ïú® ÏÉÅÌò∏ÏûëÏö©
   shap.dependence_plot(
       'M1_SME_RY_SAA_RAT',
       shap_values,
       X_test,
       interaction_index='MCT_UE_CLN_REU_RAT'
   )
   ```

4. **ÏóÖÏ¢ÖÎ≥Ñ ÏúÑÌóò Ìå®ÌÑ¥ Î∂ÑÏÑù**
   ```python
   # ÏóÖÏ¢ÖÎ≥Ñ Ï£ºÏöî ÏúÑÌóò ÏöîÏù∏
   for industry in df['HPSN_MCT_ZCD_NM'].unique():
       industry_data = df[df['HPSN_MCT_ZCD_NM'] == industry]
       industry_idx = industry_data.index

       industry_shap = shap_values[industry_idx]
       top_features = np.abs(industry_shap).mean(axis=0).argsort()[-5:]

       print(f"\n{industry} ÏóÖÏ¢Ö ÏúÑÌóò ÏöîÏù∏:")
       print(X_test.columns[top_features])
   ```

5. **ÏúÑÌóò Ïú†Ìòï Î∂ÑÎ•ò**
   ```python
   def classify_risk_type(shap_row, feature_names):
       top_risk = feature_names[np.abs(shap_row).argsort()[-3:]]

       if 'sales_' in ' '.join(top_risk):
           return 'Îß§Ï∂ú Í∏âÎùΩÌòï'
       elif 'customer_' in ' '.join(top_risk) or 'revisit' in ' '.join(top_risk):
           return 'Í≥†Í∞ù Ïù¥ÌÉàÌòï'
       elif 'delivery' in ' '.join(top_risk):
           return 'Î∞∞Îã¨ ÏùòÏ°¥Ìòï'
       elif 'rank_' in ' '.join(top_risk):
           return 'Í≤ΩÏüÅ Ïó¥ÏúÑÌòï'
       else:
           return 'Ï¢ÖÌï© ÏúÑÍ∏∞Ìòï'

   df_test['risk_type'] = [
       classify_risk_type(shap_values[i], X_test.columns)
       for i in range(len(shap_values))
   ]

   print(df_test['risk_type'].value_counts())
   ```

6. **Partial Dependence Plot**
   ```python
   from sklearn.inspection import partial_dependence, PartialDependenceDisplay

   features = ['M1_SME_RY_SAA_RAT', 'MCT_UE_CLN_REU_RAT', 'rank_change_industry']

   fig, ax = plt.subplots(figsize=(14, 4))
   PartialDependenceDisplay.from_estimator(
       xgb_model, X_test, features, ax=ax
   )
   plt.tight_layout()
   plt.show()
   ```

#### ÏÇ∞Ï∂úÎ¨º
- `notebooks/08_shap_analysis.ipynb`
- `notebooks/09_insights.ipynb`
- `pipeline/visualization/shap_plots.py`
- `results/feature_importance.csv`
- `results/risk_patterns_by_industry.csv`
- `results/shap_summary.png`
- `results/risk_type_classification.csv`

---

### Week 6: ÎπÑÏ¶àÎãàÏä§ Ï†úÏïà Î∞è Í≤∞Í≥ºÎ¨º Ï†ïÎ¶¨ (12/23 ~ 12/29)

#### Î™©Ìëú
- Í∏àÏúµÏÉÅÌíà Îß§Ïπ≠
- ÏµúÏ¢Ö Í≤∞Í≥ºÎ¨º Ï†ïÎ¶¨
- Î∞úÌëú ÏûêÎ£å Ï§ÄÎπÑ

#### ÏûëÏóÖ ÎÇ¥Ïó≠

1. **ÏúÑÌóò Ïú†ÌòïÎ≥Ñ Í∏àÏúµÏÉÅÌíà Îß§Ïπ≠**
   ```python
   # ÏúÑÌóò Ïú†ÌòïÎ≥Ñ ÎßûÏ∂§ ÏÜîÎ£®ÏÖò
   risk_solution_map = {
       'Îß§Ï∂ú Í∏âÎùΩÌòï': {
           'product': 'ÎßàÏºÄÌåÖ ÏßÄÏõê ÎåÄÏ∂ú',
           'strategy': [
               'Ïò®ÎùºÏù∏ ÎßàÏºÄÌåÖ ÎπÑÏö© ÏßÄÏõê',
               'Î∞∞Îã¨ ÌîåÎû´Ìèº ÏûÖÏ†ê ÏßÄÏõê',
               'ÌîÑÎ°úÎ™®ÏÖò Ïö¥ÏòÅ ÏûêÍ∏à'
           ],
           'expected_effect': 'Ïã†Í∑ú Í≥†Í∞ù Ïú†ÏûÖ Ï¶ùÍ∞Ä, Îß§Ï∂ú 15% ÌöåÎ≥µ'
       },
       'Í≥†Í∞ù Ïù¥ÌÉàÌòï': {
           'product': 'Í≥†Í∞ù Î¶¨ÌÖêÏÖò Ïª®ÏÑ§ÌåÖ + CRM ÏãúÏä§ÌÖú',
           'strategy': [
               'Ïû¨Î∞©Î¨∏ Í≥†Í∞ù Ìè¨Ïù∏Ìä∏ Ï†ÅÎ¶Ω',
               'Îã®Í≥® Í≥†Í∞ù Ï†ÑÏö© ÌòúÌÉù',
               'VIP Í≥†Í∞ù Í¥ÄÎ¶¨ ÌîÑÎ°úÍ∑∏Îû®'
           ],
           'expected_effect': 'Ïû¨Î∞©Î¨∏Ïú® 20% Ìñ•ÏÉÅ'
       },
       'Î∞∞Îã¨ ÏùòÏ°¥Ìòï': {
           'product': 'Ïò§ÌîÑÎùºÏù∏ Í∞ïÌôî ÌîÑÎ°úÍ∑∏Îû®',
           'strategy': [
               'Îß§Ïû• Î¶¨Îâ¥Ïñº ÏßÄÏõê ÎåÄÏ∂ú',
               'ÌôÄ ÏÑúÎπÑÏä§ Í∞úÏÑ† Ïª®ÏÑ§ÌåÖ',
               'Ìè¨Ïû• Ïö©Í∏∞ Í∞úÏÑ† ÏßÄÏõê'
           ],
           'expected_effect': 'Ïò§ÌîÑÎùºÏù∏ Îß§Ï∂ú ÎπÑÏ§ë 30% Ï¶ùÍ∞Ä'
       },
       'Í≤ΩÏüÅ Ïó¥ÏúÑÌòï': {
           'product': 'Í≤ΩÏüÅÎ†• Í∞ïÌôî Ìå®ÌÇ§ÏßÄ',
           'strategy': [
               'Î©îÎâ¥ Ï∞®Î≥ÑÌôî Ïª®ÏÑ§ÌåÖ',
               'Í∞ÄÍ≤© Í≤ΩÏüÅÎ†• Î∂ÑÏÑù',
               'ÏõêÍ∞Ä Ï†àÍ∞ê Î∞©Ïïà Ï†úÏãú'
           ],
           'expected_effect': 'ÏóÖÏ¢Ö ÎÇ¥ ÏàúÏúÑ 20% ÏÉÅÏäπ'
       },
       'Ï¢ÖÌï© ÏúÑÍ∏∞Ìòï': {
           'product': 'Í≤ΩÏòÅ ÏïàÏ†ïÌôî ÌäπÎ≥Ñ Ìå®ÌÇ§ÏßÄ',
           'strategy': [
               'Í∏¥Í∏â Ïö¥ÏòÅ ÏûêÍ∏à ÏßÄÏõê',
               'Ï†ÑÎ¨∏Í∞Ä 1:1 Ïª®ÏÑ§ÌåÖ',
               'ÏóÖÏ¢Ö Ï†ÑÌôò ÏßÄÏõê'
           ],
           'expected_effect': 'ÌèêÏóÖ ÏúÑÌóò 50% Í∞êÏÜå'
       }
   }

   # Í∞ÄÎßπÏ†êÎ≥Ñ Ï∂îÏ≤ú ÏÉÅÌíà
   df_recommendation = df_test[df_test['y_pred'] == 1].copy()
   df_recommendation['recommended_product'] = df_recommendation['risk_type'].map(
       lambda x: risk_solution_map[x]['product']
   )
   ```

2. **ROI ÏòàÏ∏°**
   ```python
   # Í∞úÏûÖ Ìö®Í≥º ÏãúÎÆ¨Î†àÏù¥ÏÖò
   def calculate_roi(risk_type, avg_monthly_sales):
       solution = risk_solution_map[risk_type]

       # ÎπÑÏö© (ÌèâÍ∑†)
       cost = {
           'ÎßàÏºÄÌåÖ ÏßÄÏõê ÎåÄÏ∂ú': 500_000,
           'Í≥†Í∞ù Î¶¨ÌÖêÏÖò Ïª®ÏÑ§ÌåÖ + CRM ÏãúÏä§ÌÖú': 300_000,
           'Ïò§ÌîÑÎùºÏù∏ Í∞ïÌôî ÌîÑÎ°úÍ∑∏Îû®': 1_000_000,
           'Í≤ΩÏüÅÎ†• Í∞ïÌôî Ìå®ÌÇ§ÏßÄ': 700_000,
           'Í≤ΩÏòÅ ÏïàÏ†ïÌôî ÌäπÎ≥Ñ Ìå®ÌÇ§ÏßÄ': 2_000_000
       }[solution['product']]

       # ÏòàÏÉÅ Îß§Ï∂ú Ï¶ùÍ∞Ä (6Í∞úÏõî)
       expected_increase = {
           'Îß§Ï∂ú Í∏âÎùΩÌòï': avg_monthly_sales * 0.15 * 6,
           'Í≥†Í∞ù Ïù¥ÌÉàÌòï': avg_monthly_sales * 0.10 * 6,
           'Î∞∞Îã¨ ÏùòÏ°¥Ìòï': avg_monthly_sales * 0.12 * 6,
           'Í≤ΩÏüÅ Ïó¥ÏúÑÌòï': avg_monthly_sales * 0.08 * 6,
           'Ï¢ÖÌï© ÏúÑÍ∏∞Ìòï': avg_monthly_sales * 0.20 * 6  # ÌèêÏóÖ ÌöåÌîº
       }[risk_type]

       roi = (expected_increase - cost) / cost * 100
       return roi

   df_recommendation['estimated_roi'] = df_recommendation.apply(
       lambda row: calculate_roi(row['risk_type'], row['avg_sales']),
       axis=1
   )
   ```

3. **Ï°∞Í∏∞ Í≤ΩÎ≥¥ ÏÑ±Í≥µ ÏÇ¨Î°Ä**
   ```python
   # Ïã§Ï†ú ÌèêÏóÖ Í∞ÄÎßπÏ†ê Ï§ë ÏòàÏ∏° ÏÑ±Í≥µ ÏÇ¨Î°Ä
   success_cases = df_test[
       (df_test['is_closed'] == 1) &
       (df_test['y_pred'] == 1)
   ].copy()

   # Lead time Î∂ÑÏÑù
   success_cases['lead_time_days'] = (
       success_cases['MCT_ME_D'] -
       pd.to_datetime(success_cases['TA_YM'], format='%Y%m')
   ).dt.days

   print(f"Ï°∞Í∏∞ Í≤ΩÎ≥¥ ÏÑ±Í≥µÎ•†: {len(success_cases) / df_test['is_closed'].sum():.2%}")
   print(f"ÌèâÍ∑† Î¶¨Îìú ÌÉÄÏûÑ: {success_cases['lead_time_days'].mean():.0f}Ïùº")

   # ÏÇ¨Î°Ä Ï†ïÎ¶¨
   for idx, case in success_cases.head(5).iterrows():
       print(f"\n[ÏÇ¨Î°Ä {idx}]")
       print(f"ÏóÖÏ¢Ö: {case['HPSN_MCT_ZCD_NM']}")
       print(f"ÏúÑÌóò Ïú†Ìòï: {case['risk_type']}")
       print(f"Ï£ºÏöî ÏúÑÌóò Ïã†Ìò∏: {case['top_risk_signals']}")
       print(f"Î¶¨Îìú ÌÉÄÏûÑ: {case['lead_time_days']}Ïùº")
   ```

4. **ÏµúÏ¢Ö Î≥¥Í≥†ÏÑú ÏûëÏÑ±**
   ```markdown
   # Í∞ÄÎßπÏ†ê ÏúÑÍ∏∞ Ï°∞Í∏∞ Í≤ΩÎ≥¥ ÏãúÏä§ÌÖú ÏµúÏ¢Ö Î≥¥Í≥†ÏÑú

   ## 1. Executive Summary
   - Î™®Îç∏ ÏÑ±Îä•: PR-AUC 0.XX, Recall@10% 0.XX
   - Ï°∞Í∏∞ Í≤ΩÎ≥¥ ÏÑ±Í≥µÎ•†: XX%
   - ÌèâÍ∑† Î¶¨Îìú ÌÉÄÏûÑ: XXÏùº

   ## 2. Ï£ºÏöî ÏúÑÍ∏∞ Ïã†Ìò∏ Top 10
   1. Ïû¨Î∞©Î¨∏Ïú® 3Í∞úÏõî Ïó∞ÏÜç ÌïòÎùΩ
   2. ÏóÖÏ¢Ö ÎÇ¥ Îß§Ï∂ú ÏàúÏúÑ 20% ÌïòÎùΩ
   ...

   ## 3. ÏóÖÏ¢ÖÎ≥Ñ ÏúÑÌóò Ìå®ÌÑ¥
   - ÏπòÌÇ®: Î∞∞Îã¨ ÏùòÏ°¥ÎèÑ Ï¶ùÍ∞Ä + Í∞ùÎã®Í∞Ä ÌïòÎùΩ
   - Ïπ¥Ìéò: Ïã†Í∑ú Í≥†Í∞ù Ïú†ÏûÖ Í∞êÏÜå + Í≤ΩÏüÅ Ïã¨Ìôî
   ...

   ## 4. ÎßûÏ∂§Ìòï Í∏àÏúµÏÉÅÌíà Ï†úÏïà
   ...

   ## 5. Í∏∞ÎåÄÌö®Í≥º
   - ÌèêÏóÖ ÏòàÎ∞©: Ïó∞Í∞Ñ XXÍ∞ú Í∞ÄÎßπÏ†ê
   - Í≤ΩÏ†úÏ†Å Ìö®Í≥º: XXÏñµ Ïõê
   ```

5. **Î∞úÌëú ÏûêÎ£å (PPT)**
   - Î¨∏Ï†ú Ï†ïÏùò Î∞è Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù
   - Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò Î∞è ÏÑ±Îä•
   - SHAP Î∂ÑÏÑù Í≤∞Í≥º (Ïù∏ÏÇ¨Ïù¥Ìä∏)
   - ÎπÑÏ¶àÎãàÏä§ Ï†úÏïà
   - Í∏∞ÎåÄÌö®Í≥º Î∞è Ìñ•ÌõÑ Í≥ÑÌöç

#### ÏÇ∞Ï∂úÎ¨º
- `notebooks/10_business_proposal.ipynb`
- `reports/final_report.pdf`
- `reports/presentation.pptx`
- `results/recommendation_by_merchant.csv`
- `results/roi_simulation.csv`
- `results/success_cases.csv`

---

## 4. Í∏∞Ïà† Ïä§ÌÉù

### 4.1 Core Libraries

```python
# Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
import pandas as pd
import numpy as np
from datetime import datetime

# Î™®Îç∏ÎßÅ
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier  # ÏòµÏÖò

# Î∂àÍ∑†Ìòï Ï≤òÎ¶¨
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import TomekLinks
from imblearn.combine import SMOTETomek

# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù
import optuna
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit

# ÏïôÏÉÅÎ∏î
from sklearn.ensemble import VotingClassifier, StackingClassifier

# ÌèâÍ∞Ä
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    precision_recall_curve,
    auc,
    fbeta_score,
    roc_auc_score
)

# Ìï¥ÏÑù
import shap
from lime import lime_tabular
from sklearn.inspection import partial_dependence, PartialDependenceDisplay

# ÏãúÍ∞ÅÌôî
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Ïú†Ìã∏Î¶¨Ìã∞
from scipy.stats import entropy
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib
import pickle
```

### 4.2 Í∞úÎ∞ú ÌôòÍ≤Ω

```toml
# pyproject.toml
[project]
name = "shinhan-202510"
version = "1.0.0"
requires-python = ">=3.12"

dependencies = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "xgboost>=2.0.0",
    "lightgbm>=4.0.0",
    "catboost>=1.2.0",
    "imbalanced-learn>=0.11.0",
    "optuna>=3.3.0",
    "shap>=0.43.0",
    "lime>=0.2.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "plotly>=5.17.0",
    "scipy>=1.11.0",
    "jupyter>=1.0.0",
    "notebook>=7.0.0",
    "ipywidgets>=8.1.0"
]
```

---

## 5. Íµ¨ÌòÑ ÏÑ∏Î∂ÄÏÇ¨Ìï≠

### 5.1 Î∂àÍ∑†Ìòï Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Ï†ÑÎûµ

#### Ï†ÑÎûµ ÎπÑÍµê

| Î∞©Î≤ï | Ïû•Ï†ê | Îã®Ï†ê | Ï∂îÏ≤ú |
|------|------|------|------|
| **SMOTE** | ÏÜåÏàò ÌÅ¥ÎûòÏä§ Ìï©ÏÑ± ÏÉùÏÑ± | Í≥ºÏ†ÅÌï© ÏúÑÌóò | ‚≠ê |
| **ADASYN** | Í≤ΩÍ≥Ñ ÏòÅÏó≠ ÏßëÏ§ë ÏÉùÏÑ± | ÎÖ∏Ïù¥Ï¶à ÎØºÍ∞ê | ‚ñ≥ |
| **Tomek Links** | Í≤ΩÍ≥Ñ Ï†ïÎ¶¨ | Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ | - |
| **SMOTETomek** | ÌïòÏù¥Î∏åÎ¶¨Îìú | Î≥µÏû°ÏÑ± Ï¶ùÍ∞Ä | ‚≠ê |
| **Class Weight** | Í∞ÑÎã®, Îπ†Î¶Ñ | ÏÑ±Îä• Ï†úÌïúÏ†Å | ‚≠ê |

#### Íµ¨ÌòÑ ÏΩîÎìú

```python
# 1. SMOTE (Ï∂îÏ≤ú)
from imblearn.over_sampling import SMOTE

smote = SMOTE(
    sampling_strategy=0.3,  # ÏÜåÏàò:Îã§Ïàò = 3:10
    k_neighbors=5,
    random_state=42
)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print(f"Before SMOTE: {y_train.value_counts()}")
print(f"After SMOTE: {y_train_sm.value_counts()}")

# 2. Class Weight (XGBoost/LightGBM)
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

xgb_params = {
    'scale_pos_weight': scale_pos_weight,  # ~32
    ...
}

lgb_params = {
    'is_unbalance': True,
    # ÎòêÎäî 'class_weight': 'balanced'
    ...
}

# 3. SMOTETomek (Í≥†Í∏â)
from imblearn.combine import SMOTETomek

smt = SMOTETomek(random_state=42)
X_train_smt, y_train_smt = smt.fit_resample(X_train, y_train)
```

### 5.2 ÏãúÍ≥ÑÏó¥ ÌäπÏÑ± ÏÉùÏÑ± Î≤†Ïä§Ìä∏ ÌîÑÎûôÌã∞Ïä§

#### Data Leakage Î∞©ÏßÄ

```python
# ‚ùå ÏûòÎ™ªÎêú Ïòà (ÎØ∏Îûò Ï†ïÎ≥¥ ÎàÑÏ∂ú)
df['sales_ma_3m'] = df['sales'].rolling(3).mean()  # ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞ Ìè¨Ìï®

# ‚úÖ Ïò¨Î∞îÎ•∏ Ïòà (Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©)
df['sales_ma_3m'] = df.groupby('ENCODED_MCT')['sales'].transform(
    lambda x: x.shift(1).rolling(3, min_periods=1).mean()
)

# ÎòêÎäî
df['sales_lag_1'] = df.groupby('ENCODED_MCT')['sales'].shift(1)
df['sales_ma_3m'] = df['sales_lag_1'].rolling(3, min_periods=1).mean()
```

#### Ìö®Ïú®Ï†ÅÏù∏ ÌäπÏÑ± ÏÉùÏÑ±

```python
def create_time_features(df, target_col, mct_col='ENCODED_MCT'):
    """ÏãúÍ≥ÑÏó¥ ÌäπÏÑ± ÏùºÍ¥Ñ ÏÉùÏÑ±"""

    features = df.copy()

    # Lag features
    for lag in [1, 3, 6]:
        features[f'{target_col}_lag_{lag}'] = features.groupby(mct_col)[target_col].shift(lag)

    # Rolling statistics
    for window in [3, 6]:
        features[f'{target_col}_ma_{window}'] = features.groupby(mct_col)[target_col].transform(
            lambda x: x.shift(1).rolling(window, min_periods=1).mean()
        )
        features[f'{target_col}_std_{window}'] = features.groupby(mct_col)[target_col].transform(
            lambda x: x.shift(1).rolling(window, min_periods=1).std()
        )

    # Change rates
    features[f'{target_col}_change_1m'] = features.groupby(mct_col)[target_col].pct_change(1)
    features[f'{target_col}_change_3m'] = features.groupby(mct_col)[target_col].pct_change(3)

    # Consecutive patterns
    features[f'{target_col}_consecutive_up'] = features.groupby(mct_col)[f'{target_col}_change_1m'].apply(
        lambda x: (x > 0).astype(int).groupby((x <= 0).cumsum()).cumsum()
    )
    features[f'{target_col}_consecutive_down'] = features.groupby(mct_col)[f'{target_col}_change_1m'].apply(
        lambda x: (x < 0).astype(int).groupby((x >= 0).cumsum()).cumsum()
    )

    return features

# ÏÇ¨Ïö© Ïòà
df = create_time_features(df, 'RC_M1_SAA')
df = create_time_features(df, 'RC_M1_UE_CUS_CN')
```

### 5.3 Î™®Îç∏ ÌïôÏäµ Î∞è Í≤ÄÏ¶ù

#### Early Stopping ÌôúÏö©

```python
# XGBoost
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_valid, y_valid)],
    eval_metric=['aucpr', 'logloss'],
    early_stopping_rounds=50,
    verbose=50
)

# ÏµúÏ†Å iteration ÌôïÏù∏
best_iteration = xgb_model.best_iteration
print(f"Best iteration: {best_iteration}")

# LightGBM
lgb_model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_valid, y_valid)],
    eval_metric='auc',
    callbacks=[
        lgb.early_stopping(50),
        lgb.log_evaluation(50)
    ]
)
```

#### ÏãúÍ≥ÑÏó¥ ÍµêÏ∞® Í≤ÄÏ¶ù

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5, test_size=3)  # 3Í∞úÏõîÏî© Í≤ÄÏ¶ù

cv_scores = []
for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
    print(f"\nFold {fold + 1}")

    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]

    model = xgb.XGBClassifier(**best_params)
    model.fit(X_tr, y_tr,
              eval_set=[(X_val, y_val)],
              early_stopping_rounds=30,
              verbose=False)

    y_proba = model.predict_proba(X_val)[:, 1]
    precision, recall, _ = precision_recall_curve(y_val, y_proba)
    pr_auc = auc(recall, precision)

    cv_scores.append(pr_auc)
    print(f"PR-AUC: {pr_auc:.4f}")

print(f"\nAverage PR-AUC: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}")
```

### 5.4 ÏïôÏÉÅÎ∏î Ï†ÑÎûµ

#### Weighted Voting

```python
# Í≤ÄÏ¶ù ÏÑ±Îä• Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
xgb_score = pr_auc_xgb  # Ïòà: 0.65
lgb_score = pr_auc_lgb  # Ïòà: 0.63

total_score = xgb_score + lgb_score
xgb_weight = xgb_score / total_score  # 0.51
lgb_weight = lgb_score / total_score  # 0.49

# Voting Classifier
voting_clf = VotingClassifier(
    estimators=[
        ('xgb', xgb_model),
        ('lgb', lgb_model)
    ],
    voting='soft',
    weights=[xgb_weight, lgb_weight]
)

voting_clf.fit(X_train, y_train)
y_proba_ensemble = voting_clf.predict_proba(X_test)[:, 1]
```

#### Stacking

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Meta-learnerÏóê ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò Ï†ÅÏö©
meta_learner = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    random_state=42
)

stacking_clf = StackingClassifier(
    estimators=[
        ('xgb', xgb_model),
        ('lgb', lgb_model)
    ],
    final_estimator=meta_learner,
    cv=TimeSeriesSplit(n_splits=5),
    passthrough=False  # Base model ÏòàÏ∏°Îßå ÏÇ¨Ïö©
)

stacking_clf.fit(X_train, y_train)
```

---

## 6. ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞

```
shinhan_202510/
‚îÇ
‚îú‚îÄ‚îÄ data/                           # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞
‚îÇ   ‚îú‚îÄ‚îÄ big_data_set1_f.csv
‚îÇ   ‚îú‚îÄ‚îÄ big_data_set2_f.csv
‚îÇ   ‚îî‚îÄ‚îÄ big_data_set3_f.csv
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                      # Jupyter ÎÖ∏Ìä∏Î∂Å
‚îÇ   ‚îú‚îÄ‚îÄ 00_temp.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb               # EDA (ÏôÑÎ£å)
‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb     # Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
‚îÇ   ‚îú‚îÄ‚îÄ 03_feature_engineering.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_model_xgboost.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 05_model_lightgbm.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 06_hyperparameter_tuning.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 07_ensemble.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 08_shap_analysis.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 09_insights.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 10_business_proposal.ipynb
‚îÇ
‚îú‚îÄ‚îÄ pipeline/                      # ML ÌååÏù¥ÌîÑÎùºÏù∏ Î™®Îìà
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py        # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_encoder.py    # Íµ¨Í∞Ñ Ïù∏ÏΩîÎî©
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ missing_handler.py    # Í≤∞Ï∏°Í∞í Ï≤òÎ¶¨
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ time_series_features.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ customer_features.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ composite_features.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_selector.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xgboost_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lightgbm_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ensemble.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_utils.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py            # Custom metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validators.py         # CV Ï†ÑÎûµ
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ visualization/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ shap_plots.py
‚îÇ       ‚îú‚îÄ‚îÄ performance_plots.py
‚îÇ       ‚îî‚îÄ‚îÄ business_plots.py
‚îÇ
‚îú‚îÄ‚îÄ models/                        # ÌïôÏäµÎêú Î™®Îç∏ Ï†ÄÏû•
‚îÇ   ‚îú‚îÄ‚îÄ xgb_baseline.pkl
‚îÇ   ‚îú‚îÄ‚îÄ lgb_baseline.pkl
‚îÇ   ‚îú‚îÄ‚îÄ xgb_tuned.pkl
‚îÇ   ‚îú‚îÄ‚îÄ lgb_tuned.pkl
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_voting.pkl
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_stacking.pkl
‚îÇ   ‚îî‚îÄ‚îÄ best_model.pkl
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Î∂ÑÏÑù Í≤∞Í≥º
‚îÇ   ‚îú‚îÄ‚îÄ feature_importance.csv
‚îÇ   ‚îú‚îÄ‚îÄ shap_values.npy
‚îÇ   ‚îú‚îÄ‚îÄ risk_patterns_by_industry.csv
‚îÇ   ‚îú‚îÄ‚îÄ risk_type_classification.csv
‚îÇ   ‚îú‚îÄ‚îÄ recommendation_by_merchant.csv
‚îÇ   ‚îú‚îÄ‚îÄ roi_simulation.csv
‚îÇ   ‚îú‚îÄ‚îÄ success_cases.csv
‚îÇ   ‚îî‚îÄ‚îÄ visualizations/
‚îÇ       ‚îú‚îÄ‚îÄ shap_summary.png
‚îÇ       ‚îú‚îÄ‚îÄ pr_curve.png
‚îÇ       ‚îî‚îÄ‚îÄ confusion_matrix.png
‚îÇ
‚îú‚îÄ‚îÄ reports/                       # ÏµúÏ¢Ö Î≥¥Í≥†ÏÑú
‚îÇ   ‚îú‚îÄ‚îÄ final_report.pdf
‚îÇ   ‚îú‚îÄ‚îÄ presentation.pptx
‚îÇ   ‚îî‚îÄ‚îÄ executive_summary.md
‚îÇ
‚îú‚îÄ‚îÄ docs/                          # Î¨∏ÏÑú
‚îÇ   ‚îú‚îÄ‚îÄ 00_bigcontest_2025.md
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_layout.md
‚îÇ   ‚îú‚îÄ‚îÄ 02_approach.md
‚îÇ   ‚îî‚îÄ‚îÄ 03_plan_XGBoost_LightGBM.md  # Ïù¥ Î¨∏ÏÑú
‚îÇ
‚îú‚îÄ‚îÄ configs/                       # ÏÑ§Ï†ï ÌååÏùº
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ model_params.yaml
‚îÇ
‚îú‚îÄ‚îÄ tests/                         # ÌÖåÏä§Ìä∏ ÏΩîÎìú
‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ test_features.py
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .python-version
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CLAUDE.md
‚îî‚îÄ‚îÄ main.py
```

---

## 7. ÏòàÏÉÅ Í≤∞Í≥ºÎ¨º

### 7.1 Î™®Îç∏ ÏÑ±Îä• Î™©Ìëú

| ÏßÄÌëú | Î™©ÌëúÍ∞í | Îã¨ÏÑ± Ï†ÑÎûµ |
|------|--------|-----------|
| **PR-AUC** | > 0.55 | ÏïôÏÉÅÎ∏î + ÏãúÍ≥ÑÏó¥ ÌäπÏÑ± |
| **Recall@10%** | > 0.70 | Î∂àÍ∑†Ìòï Ï≤òÎ¶¨ + ÏûÑÍ≥ÑÍ∞í ÏµúÏ†ÅÌôî |
| **Precision@10%** | > 0.60 | Feature Engineering |
| **F2-score** | > 0.60 | Recall Ï§ëÏãú ÌäúÎãù |
| **ÌèâÍ∑† Î¶¨Îìú ÌÉÄÏûÑ** | > 60Ïùº | Ï°∞Í∏∞ ÌÉÄÍ≤ü Î≥ÄÏàò (3Í∞úÏõî Ï†Ñ) |

### 7.2 ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏ (ÏòàÏÉÅ)

#### Ï£ºÏöî ÏúÑÍ∏∞ Ïã†Ìò∏ Top 10
1. **Ïû¨Î∞©Î¨∏Ïú® 3Í∞úÏõî Ïó∞ÏÜç ÌïòÎùΩ** (SHAP: 0.25)
2. **ÏóÖÏ¢Ö ÎÇ¥ Îß§Ï∂ú ÏàúÏúÑ 20% ÌïòÎùΩ** (SHAP: 0.18)
3. **Ïã†Í∑ú Í≥†Í∞ù Ïú†ÏûÖ 6Í∞úÏõî Í∞êÏÜå** (SHAP: 0.15)
4. **Î∞∞Îã¨ ÏùòÏ°¥ÎèÑ Í∏âÏ¶ù + Ï¥ù Îß§Ï∂ú Í∞êÏÜå** (SHAP: 0.12)
5. **Í∞ùÎã®Í∞Ä ÏßÄÏÜç ÌïòÎùΩ** (SHAP: 0.10)
6. **ÏÉÅÍ∂å ÎÇ¥ Ìï¥ÏßÄÏú® Ï¶ùÍ∞Ä** (SHAP: 0.09)
7. **Îß§Ï∂ú Î≥ÄÎèôÏÑ± Í∏âÏ¶ù** (SHAP: 0.08)
8. **Ïö¥ÏòÅ Ï¥àÍ∏∞ 6Í∞úÏõî + Îß§Ï∂ú Î∂ÄÏßÑ** (SHAP: 0.07)
9. **Í≥†Í∞ù Îã§ÏñëÏÑ± Í∞êÏÜå** (SHAP: 0.06)
10. **Ïú†ÎèôÏù∏Íµ¨ Í≥†Í∞ù ÎπÑÏ§ë Í∏âÍ∞ê** (SHAP: 0.05)

#### ÏóÖÏ¢ÖÎ≥Ñ ÏúÑÌóò Ìå®ÌÑ¥
- **ÏπòÌÇ®**: Î∞∞Îã¨ ÏùòÏ°¥ÎèÑ ‚Üë + Í∞ùÎã®Í∞Ä ‚Üì
- **Ïπ¥Ìéò**: Ïã†Í∑ú Í≥†Í∞ù Ïú†ÏûÖ ‚Üì + Í≤ΩÏüÅ Ïã¨Ìôî
- **ÌïúÏãù**: Ïû¨Î∞©Î¨∏Ïú® ‚Üì + Í≥†Í∞ù Í≥†Î†πÌôî
- **ÏùºÏãù**: Í≥†Í∞Ä Î©îÎâ¥ ÌåêÎß§ ‚Üì + Î≥ÄÎèôÏÑ± ‚Üë

### 7.3 ÎπÑÏ¶àÎãàÏä§ Ï†úÏïà

#### ÏúÑÌóò Ïú†ÌòïÎ≥Ñ Í∏àÏúµÏÉÅÌíà

| ÏúÑÌóò Ïú†Ìòï | Ï∂îÏ≤ú ÏÉÅÌíà | Í∏∞ÎåÄÌö®Í≥º | ÏòàÏÉÅ ROI |
|-----------|----------|----------|----------|
| Îß§Ï∂ú Í∏âÎùΩÌòï | ÎßàÏºÄÌåÖ ÏßÄÏõê ÎåÄÏ∂ú | Îß§Ï∂ú 15% ÌöåÎ≥µ | 180% |
| Í≥†Í∞ù Ïù¥ÌÉàÌòï | CRM ÏãúÏä§ÌÖú ÏßÄÏõê | Ïû¨Î∞©Î¨∏Ïú® 20% ‚Üë | 150% |
| Î∞∞Îã¨ ÏùòÏ°¥Ìòï | Ïò§ÌîÑÎùºÏù∏ Í∞ïÌôî ÌîÑÎ°úÍ∑∏Îû® | Ïò§ÌîÑÎùºÏù∏ Îß§Ï∂ú 30% ‚Üë | 120% |
| Í≤ΩÏüÅ Ïó¥ÏúÑÌòï | Í≤ΩÏüÅÎ†• Í∞ïÌôî Ìå®ÌÇ§ÏßÄ | ÏàúÏúÑ 20% ÏÉÅÏäπ | 140% |
| Ï¢ÖÌï© ÏúÑÍ∏∞Ìòï | Í≤ΩÏòÅ ÏïàÏ†ïÌôî Ìå®ÌÇ§ÏßÄ | ÌèêÏóÖ ÏúÑÌóò 50% ‚Üì | 250% |

#### Í≤ΩÏ†úÏ†Å Ìö®Í≥º (ÏòàÏÉÅ)
- **ÌèêÏóÖ ÏòàÎ∞©**: Ïó∞Í∞Ñ 50~80Í∞ú Í∞ÄÎßπÏ†ê
- **Îß§Ï∂ú ÌöåÎ≥µ**: Í∞ÄÎßπÏ†êÎãπ ÌèâÍ∑† 500ÎßåÏõê/Ïõî √ó 6Í∞úÏõî = 3,000ÎßåÏõê
- **Ï¥ù Í≤ΩÏ†úÏ†Å Ìö®Í≥º**: 15Ïñµ ~ 24Ïñµ Ïõê/ÎÖÑ
- **Í∏àÏúµÏÉÅÌíà ÎåÄÏ∂úÏï°**: Í∞ÄÎßπÏ†êÎãπ ÌèâÍ∑† 100ÎßåÏõê (Ï¥ù 5Ï≤úÎßå~8Ï≤úÎßåÏõê)
- **ÏòàÏÉÅ ÏàòÏùµ**: ÎåÄÏ∂ú Ïù¥Ïûê + Ïª®ÏÑ§ÌåÖ ÏàòÏàòÎ£å = 1Ïñµ ~ 1.5Ïñµ Ïõê/ÎÖÑ

### 7.4 ÏµúÏ¢Ö Ï†úÏ∂úÎ¨º

#### 1. Î∂ÑÏÑù Î≥¥Í≥†ÏÑú (PDF)
- Executive Summary (2ÌéòÏù¥ÏßÄ)
- Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù (10ÌéòÏù¥ÏßÄ)
- Î™®Îç∏ÎßÅ Í≤∞Í≥º (10ÌéòÏù¥ÏßÄ)
- Ïù∏ÏÇ¨Ïù¥Ìä∏ Î∞è Ï†úÏïà (8ÌéòÏù¥ÏßÄ)

#### 2. Î∞úÌëú ÏûêÎ£å (PPT)
- Î¨∏Ï†ú Ï†ïÏùò (3 slides)
- EDA Î∞è Ïù∏ÏÇ¨Ïù¥Ìä∏ (5 slides)
- Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò (4 slides)
- SHAP Î∂ÑÏÑù Í≤∞Í≥º (5 slides)
- ÎπÑÏ¶àÎãàÏä§ Ï†úÏïà (5 slides)
- Q&A (2 slides)

#### 3. ÏΩîÎìú Î∞è Î™®Îç∏
- GitHub Repository
- Ïû¨ÌòÑ Í∞ÄÎä•Ìïú Jupyter Notebooks
- ÌïôÏäµÎêú Î™®Îç∏ ÌååÏùº (.pkl)
- README with Ïã§Ìñâ Í∞ÄÏù¥Îìú

#### 4. Î∂ÄÎ°ù
- Feature Î™©Î°ù Î∞è Ï†ïÏùò
- ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù Î°úÍ∑∏
- Ï∂îÍ∞Ä ÏãúÍ∞ÅÌôî ÏûêÎ£å

---

## 8. ÏúÑÌóò ÏöîÏÜå Î∞è ÎåÄÏùë Î∞©Ïïà

### 8.1 Ïû†Ïû¨Ï†Å ÏúÑÌóò

| ÏúÑÌóò | ÏòÅÌñ• | ÌôïÎ•† | ÎåÄÏùë Î∞©Ïïà |
|------|------|------|-----------|
| **Îç∞Ïù¥ÌÑ∞ Î∂àÍ∑†Ìòï Ïã¨Ìôî** | ÏÑ±Îä• Ï†ÄÌïò | ÎÜíÏùå | SMOTE + Class Weight Î≥ëÌñâ |
| **Í≥ºÏ†ÅÌï©** | ÏùºÎ∞òÌôî Ïã§Ìå® | Ï§ëÍ∞Ñ | Early Stopping + CV |
| **Feature ÎàÑÏ∂ú** | ÎπÑÌòÑÏã§Ï†Å ÏÑ±Îä• | Ï§ëÍ∞Ñ | Lag featuresÎßå ÏÇ¨Ïö© Í≤ÄÏ¶ù |
| **Ìï¥ÏÑù Î≥µÏû°ÎèÑ** | Ïù∏ÏÇ¨Ïù¥Ìä∏ Î∂ÄÏ°± | ÎÇÆÏùå | SHAP + ÎèÑÎ©îÏù∏ Ï†ÑÎ¨∏Í∞Ä ÌòëÏóÖ |
| **Í≥ÑÏÇ∞ ÏãúÍ∞Ñ Î∂ÄÏ°±** | ÌäúÎãù Ï†úÌïú | Ï§ëÍ∞Ñ | Optuna Ìö®Ïú®Ï†Å ÌÉêÏÉâ |

### 8.2 ÎåÄÏùë Ï†ÑÎûµ

1. **ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ**
   - Train/Valid/Test ÏÑ±Îä• ÏßÄÏÜç Ï∂îÏ†Å
   - Overfitting Ï°∞Í∏∞ Î∞úÍ≤¨

2. **Î∞±ÏóÖ ÌîåÎûú**
   - XGBoost Îã®ÎèÖ Î™®Îç∏ ÌôïÎ≥¥
   - Í∞ÑÎã®Ìïú Voting ÏïôÏÉÅÎ∏î ÎåÄÏïà

3. **ÏãúÍ∞Ñ Í¥ÄÎ¶¨**
   - Ï£ºÏ∞®Î≥Ñ ÎßàÏùºÏä§ÌÜ§ ÏóÑÏàò
   - Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò ÏûëÏóÖ

---

## 9. ÏÑ±Í≥µ Í∏∞Ï§Ä

### 9.1 Í∏∞Ïà†Ï†Å ÏÑ±Í≥µ

- [ ] PR-AUC > 0.55
- [ ] Recall@10% > 0.70
- [ ] ÌèâÍ∑† Î¶¨Îìú ÌÉÄÏûÑ > 60Ïùº
- [ ] SHAP Î∂ÑÏÑù ÏôÑÎ£å
- [ ] Ïû¨ÌòÑ Í∞ÄÎä•Ìïú ÏΩîÎìú

### 9.2 ÎπÑÏ¶àÎãàÏä§ ÏÑ±Í≥µ

- [ ] ÏúÑÍ∏∞ Ïã†Ìò∏ Top 10 ÎèÑÏ∂ú
- [ ] ÏóÖÏ¢ÖÎ≥Ñ Ìå®ÌÑ¥ 5Í∞ú Ïù¥ÏÉÅ
- [ ] Í∏àÏúµÏÉÅÌíà Îß§Ïπ≠ Ï≤¥Í≥Ñ Íµ¨Ï∂ï
- [ ] ROI ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏôÑÎ£å
- [ ] Ïã§Ìñâ Í∞ÄÎä•Ìïú Ï†úÏïà

### 9.3 ÌèâÍ∞Ä Í∏∞Ï§Ä (100Ï†ê)

- **Î™®Îç∏ÎßÅ Ï†ÅÌï©ÏÑ±/ÏôÑÏÑ±ÎèÑ** (25Ï†ê): XGBoost/LightGBM ÏïôÏÉÅÎ∏î, Î∂àÍ∑†Ìòï Ï≤òÎ¶¨
- **Îç∞Ïù¥ÌÑ∞ ÌôúÏö©/Î∂ÑÏÑùÎ†•** (25Ï†ê): ÏãúÍ≥ÑÏó¥ ÌäπÏÑ±, ÎèÑÎ©îÏù∏ ÏßÄÏãù ÌôúÏö©
- **Ïù∏ÏÇ¨Ïù¥Ìä∏/Ïã§Ìö®ÏÑ±** (20Ï†ê): SHAP Î∂ÑÏÑù, ÏúÑÍ∏∞ Ïã†Ìò∏ ÎèÑÏ∂ú
- **Í∏àÏúµÏÉÅÌíà Ï†úÏïà** (20Ï†ê): ÏúÑÌóò Ïú†ÌòïÎ≥Ñ ÎßûÏ∂§ ÏÜîÎ£®ÏÖò
- **ÏôÑÏÑ±ÎèÑ** (10Ï†ê): Î≥¥Í≥†ÏÑú ÌíàÏßà, Î∞úÌëú ÏôÑÏÑ±ÎèÑ

---

## 10. Ï∞∏Í≥† ÏûêÎ£å

### 10.1 Í¥ÄÎ†® ÎÖºÎ¨∏
- Chen & Guestrin (2016). "XGBoost: A Scalable Tree Boosting System"
- Ke et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
- Chawla et al. (2002). "SMOTE: Synthetic Minority Over-sampling Technique"
- Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions (SHAP)"

### 10.2 Ïú†Ïö©Ìïú ÎßÅÌÅ¨
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [SHAP Documentation](https://shap.readthedocs.io/)
- [Imbalanced-learn Guide](https://imbalanced-learn.org/)
- [Optuna Tutorials](https://optuna.readthedocs.io/)

### 10.3 Kaggle Ï∞∏Í≥† ÏÜîÎ£®ÏÖò
- [Imbalanced Classification - Credit Card Fraud](https://www.kaggle.com/code/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets)
- [Time Series with XGBoost](https://www.kaggle.com/code/robikscube/time-series-forecasting-with-xgboost)
- [SHAP Analysis Tutorial](https://www.kaggle.com/code/dansbecker/shap-values)

---

**ÏûëÏÑ±Ïùº**: 2025-10-05
**Î≤ÑÏ†Ñ**: 1.0
**ÏûëÏÑ±Ïûê**: AI Analysis Team
**ÌîÑÎ°úÏ†ùÌä∏**: 2025 ÎπÖÏΩòÌÖåÏä§Ìä∏ - Í∞ÄÎßπÏ†ê ÏúÑÍ∏∞ Ï°∞Í∏∞ Í≤ΩÎ≥¥ ÏãúÏä§ÌÖú
