# XGBoost/LightGBM ê¸°ë°˜ ê°€ë§¹ì  ìœ„ê¸° ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ êµ¬í˜„ ê³„íš

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [êµ¬í˜„ ì•„í‚¤í…ì²˜](#2-êµ¬í˜„-ì•„í‚¤í…ì²˜)
3. [ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš](#3-ë‹¨ê³„ë³„-êµ¬í˜„-ê³„íš)
4. [ê¸°ìˆ  ìŠ¤íƒ](#4-ê¸°ìˆ -ìŠ¤íƒ)
5. [êµ¬í˜„ ì„¸ë¶€ì‚¬í•­](#5-êµ¬í˜„-ì„¸ë¶€ì‚¬í•­)
6. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#6-ë””ë ‰í† ë¦¬-êµ¬ì¡°)
7. [ì˜ˆìƒ ê²°ê³¼ë¬¼](#7-ì˜ˆìƒ-ê²°ê³¼ë¬¼)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ
**ì˜ì„¸/ì¤‘ì†Œ ìš”ì‹ ê°€ë§¹ì ì˜ ê²½ì˜ ìœ„ê¸°ë¥¼ 1~3ê°œì›” ì „ì— ì˜ˆì¸¡í•˜ëŠ” AI ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ ê°œë°œ**

### 1.2 í•µì‹¬ ê³¼ì œ
- íì—… ìœ„í—˜ ê°€ë§¹ì  ì¡°ê¸° ì‹ë³„ (Recall ìµœëŒ€í™”)
- ìœ„ê¸° ì‹ í˜¸ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ (SHAP)
- ë§ì¶¤í˜• ê¸ˆìœµìƒí’ˆ/ì„œë¹„ìŠ¤ ì œì•ˆ

### 1.3 ì™œ XGBoost/LightGBMì¸ê°€?

#### ì„ íƒ ì´ìœ 
1. **ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ìš°ìˆ˜**: íì—…ë¥  3% vs ìš´ì˜ì¤‘ 97%
2. **í•´ì„ ê°€ëŠ¥ì„±**: Feature Importance + SHAP values
3. **ê²°ì¸¡ê°’ ìë™ ì²˜ë¦¬**: SV(-999999.9) ê°’ í¬í•¨
4. **ë²”ì£¼í˜• ë°ì´í„° íš¨ê³¼ì  ì²˜ë¦¬**: 6ë‹¨ê³„ êµ¬ê°„ ë°ì´í„°
5. **ë¹ ë¥¸ í•™ìŠµ ì†ë„**: ë‹¤ì–‘í•œ ì‹¤í—˜ ê°€ëŠ¥
6. **ê²€ì¦ëœ ì„±ëŠ¥**: Kaggle ë“± ëŒ€íšŒ ìƒìœ„ê¶Œ ì†”ë£¨ì…˜

#### ì¥ì  ë¹„êµ

| íŠ¹ì„± | XGBoost | LightGBM |
|------|---------|----------|
| **í•™ìŠµ ì†ë„** | ë³´í†µ | ë¹ ë¦„ âš¡ |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨** | ë³´í†µ | ìš°ìˆ˜ |
| **ëŒ€ìš©ëŸ‰ ë°ì´í„°** | ì œí•œì  | ìš°ìˆ˜ |
| **ë¶ˆê· í˜• ì²˜ë¦¬** | `scale_pos_weight` | `is_unbalance=True` |
| **ë²”ì£¼í˜• ì²˜ë¦¬** | Label encoding í•„ìš” | ìë™ ì§€ì› |
| **ê³¼ì í•© ë°©ì§€** | ìš°ìˆ˜ | ìš°ìˆ˜ |

**â†’ ë‘ ëª¨ë¸ ëª¨ë‘ í•™ìŠµ í›„ ì•™ìƒë¸”ë¡œ ìµœì¢… ì„±ëŠ¥ ê·¹ëŒ€í™”**

---

## 2. êµ¬í˜„ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

```
[ì›ë³¸ ë°ì´í„°]
    â†“
[ë°ì´í„° ì „ì²˜ë¦¬]
- ë‚ ì§œ ë³€í™˜
- SV ì²˜ë¦¬
- êµ¬ê°„ ì¸ì½”ë”©
    â†“
[Feature Engineering]
- ì‹œê³„ì—´ íŠ¹ì„± (lag, rolling)
- ìƒëŒ€ì  ì§€í‘œ (ìˆœìœ„ ë³€í™”)
- ê³ ê° í–‰ë™ (ì¬ë°©ë¬¸ìœ¨ ì¶”ì„¸)
- ë³µí•© ì§€í‘œ (ë§¤ì¶œ-ê³ ê° ê´´ë¦¬)
    â†“
[Train/Valid/Test Split]
- ì‹œê°„ ê¸°ë°˜ ë¶„í• 
- 202401-202406: Train
- 202407-202409: Valid
- 202410-202412: Test
    â†“
[ë¶ˆê· í˜• ì²˜ë¦¬]
- SMOTE (Over-sampling)
- Class Weight ì¡°ì •
    â†“
[ëª¨ë¸ í•™ìŠµ]
- XGBoost
- LightGBM
- (CatBoost - ì˜µì…˜)
    â†“
[ì•™ìƒë¸”]
- Voting (Soft)
- Stacking
    â†“
[ëª¨ë¸ í•´ì„]
- SHAP values
- Feature Importance
- Partial Dependence Plot
    â†“
[ìœ„ê¸° ì‹ í˜¸ ë¶„ì„]
- ì£¼ìš” ìœ„í—˜ ìš”ì¸ Top 10
- ì—…ì¢…/ìƒê¶Œë³„ íŒ¨í„´
- ìœ„í—˜ ìœ í˜• ë¶„ë¥˜
    â†“
[ë¹„ì¦ˆë‹ˆìŠ¤ ì œì•ˆ]
- ê¸ˆìœµìƒí’ˆ ë§¤ì¹­
- ê°œì… ì „ëµ
```

### 2.2 ëª¨ë¸ êµ¬ì¡°

#### ê°œë³„ ëª¨ë¸
```python
# XGBoost ëª¨ë¸
xgb_model = XGBClassifier(
    objective='binary:logistic',
    scale_pos_weight=32,  # (4058/127)
    max_depth=6,
    learning_rate=0.05,
    n_estimators=1000,
    subsample=0.8,
    colsample_bytree=0.8,
    early_stopping_rounds=50
)

# LightGBM ëª¨ë¸
lgb_model = LGBMClassifier(
    objective='binary',
    is_unbalance=True,
    max_depth=6,
    learning_rate=0.05,
    n_estimators=1000,
    subsample=0.8,
    colsample_bytree=0.8,
    early_stopping_rounds=50
)
```

#### ì•™ìƒë¸” ì „ëµ
1. **Soft Voting**: í™•ë¥  í‰ê· 
2. **Stacking**: Logistic Regression meta-learner
3. **ê°€ì¤‘ í‰ê· **: ê²€ì¦ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜

---

## 3. ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš

### Week 1: ë°ì´í„° ì „ì²˜ë¦¬ ë° ê¸°ë³¸ Feature (11/18 ~ 11/24)

#### ëª©í‘œ
- 3ê°œ ë°ì´í„°ì…‹ í†µí•©
- ê²°ì¸¡ê°’(SV) ì²˜ë¦¬
- ê¸°ë³¸ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±

#### ì‘ì—… ë‚´ì—­
1. **ë°ì´í„° ë¡œë“œ ë° ë³‘í•©**
   ```python
   # Dataset 1 + 2 + 3 í†µí•©
   # Step 1: Dataset 2(ì›”ë³„ ë§¤ì¶œ/ì´ìš© í˜„í™©) + Dataset 3(ì›”ë³„ ê³ ê° ì •ë³´)ë¥¼ ê°€ë§¹ì IDì™€ ë…„ì›” ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
   #         â†’ ê°™ì€ ê°€ë§¹ì ì˜ ê°™ì€ ì›” ë°ì´í„°ë¥¼ ì˜†ìœ¼ë¡œ ë¶™ì„
   df_merged = df2.merge(df3, on=['ENCODED_MCT', 'TA_YM'])

   # Step 2: ìœ„ ê²°ê³¼ + Dataset 1(ê°€ë§¹ì  ê¸°ë³¸ì •ë³´: ì—…ì¢…, ìœ„ì¹˜, íì—…ì¼)ì„ ê°€ë§¹ì ID ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
   #         â†’ ê°€ë§¹ì ë³„ ê³ ì • ì •ë³´ë¥¼ ëª¨ë“  ì›” ë°ì´í„°ì— ì¶”ê°€
   df_full = df_merged.merge(df1[['ENCODED_MCT', ...]], on='ENCODED_MCT')

   # ìµœì¢…: ê° í–‰ì´ "íŠ¹ì • ê°€ë§¹ì ì˜ íŠ¹ì • ì›”" ë°ì´í„°ê°€ ë¨ (ì‹œê³„ì—´ ë¶„ì„ ê°€ëŠ¥)
   ```

2. **íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±**
   ```python
   # íì—… ì—¬ë¶€
   df['is_closed'] = df['MCT_ME_D'].notna().astype(int)

   # ë¯¸ë˜ Nê°œì›” ë‚´ íì—… (ì¡°ê¸° ê²½ë³´ìš©)
   df['will_close_1m'] = ...
   df['will_close_3m'] = ...
   ```

3. **ê²°ì¸¡ê°’ ì²˜ë¦¬**
   ```python
   # SV(-999999.9) â†’ NaN ë³€í™˜
   df.replace(-999999.9, np.nan, inplace=True)

   # ì „ëµ:
   # - ë°°ë‹¬ë§¤ì¶œ: 0ìœ¼ë¡œ ëŒ€ì²´ (ë°°ë‹¬ ë¯¸ìš´ì˜)
   # - ê³ ê°ì •ë³´: ì¤‘ì•™ê°’ ë˜ëŠ” ë³„ë„ í”Œë˜ê·¸ ìƒì„±
   ```

4. **êµ¬ê°„ ë°ì´í„° ì¸ì½”ë”©**
   ```python
   interval_mapping = {
       '1_10%ì´í•˜': 1,
       '2_10-25%': 2,
       '3_25-50%': 3,
       '4_50-75%': 4,
       '5_75-90%': 5,
       '6_90%ì´ˆê³¼(í•˜ìœ„ 10% ì´í•˜)': 6
   }
   ```

#### ì‚°ì¶œë¬¼
- `notebooks/02_preprocessing.ipynb`
- `pipeline/preprocessing/data_loader.py`
- `pipeline/preprocessing/feature_encoder.py`

---

### Week 2: ì‹œê³„ì—´ Feature Engineering (11/25 ~ 12/01)

#### ëª©í‘œ
- ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± (100+ features)
- Feature selection

#### ì‘ì—… ë‚´ì—­

1. **ì‹œê³„ì—´ ê¸°ë³¸ íŠ¹ì„±**
   ```python
   # Lag features
   for col in ['RC_M1_SAA', 'RC_M1_TO_UE_CT', 'RC_M1_UE_CUS_CN']:
       for lag in [1, 3, 6, 12]:
           df[f'{col}_lag_{lag}'] = df.groupby('ENCODED_MCT')[col].shift(lag)

   # ì´ë™í‰ê· 
   for window in [3, 6]:
       df[f'sales_ma_{window}m'] = df.groupby('ENCODED_MCT')['sales'].transform(
           lambda x: x.rolling(window, min_periods=1).mean()
       )

   # ì¦ê°ë¥ 
   df['sales_mom'] = df.groupby('ENCODED_MCT')['sales'].pct_change()
   df['sales_qoq'] = df.groupby('ENCODED_MCT')['sales'].pct_change(3)
   ```

2. **ì¶”ì„¸ ë° ë³€ë™ì„±**
   ```python
   # ì—°ì† í•˜ë½ ê°œì›” ìˆ˜
   df['consecutive_decline'] = ...

   # ë³€ë™ ê³„ìˆ˜
   df['cv_sales_3m'] = (
       df.groupby('ENCODED_MCT')['sales'].transform(lambda x: x.rolling(3).std()) /
       df.groupby('ENCODED_MCT')['sales'].transform(lambda x: x.rolling(3).mean())
   )
   ```

3. **ìƒëŒ€ì  ì§€í‘œ**
   ```python
   # ì—…ì¢… ë‚´ ìˆœìœ„ ë³€í™”
   df['rank_change_industry'] = df.groupby('ENCODED_MCT')['M12_SME_RY_SAA_PCE_RT'].diff()

   # ì—…ì¢… í‰ê·  ëŒ€ë¹„ ê²©ì°¨
   df['gap_from_industry'] = df['M1_SME_RY_SAA_RAT'] - 100
   ```

4. **ê³ ê° í–‰ë™ ì§€í‘œ**
   ```python
   # ì¬ë°©ë¬¸ìœ¨ ë³€í™”
   df['revisit_rate_change'] = df.groupby('ENCODED_MCT')['MCT_UE_CLN_REU_RAT'].diff()

   # ê³ ê° ë‹¤ì–‘ì„± (Entropy)
   from scipy.stats import entropy
   age_cols = ['M12_MAL_1020_RAT', 'M12_MAL_30_RAT', ...]
   df['customer_diversity'] = df[age_cols].apply(
       lambda row: entropy(row / row.sum()) if row.sum() > 0 else 0, axis=1
   )
   ```

5. **ë³µí•© ì§€í‘œ**
   ```python
   # ë§¤ì¶œ-ê³ ê° ê´´ë¦¬ë„
   df['sales_customer_gap'] = (
       df['sales_change'] - df['customer_change']
   )

   # ë°°ë‹¬ ì˜ì¡´ë„ ì¦ê°€ + ë§¤ì¶œ ê°ì†Œ
   df['delivery_risk'] = (
       (df['delivery_ratio_change'] > 10) & (df['sales_change'] < 0)
   ).astype(int)
   ```

6. **Feature Selection**
   ```python
   from sklearn.feature_selection import mutual_info_classif

   # ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ ì„ íƒ
   mi_scores = mutual_info_classif(X, y, random_state=42)
   top_features = mi_scores.argsort()[-100:]  # ìƒìœ„ 100ê°œ
   ```

#### ì‚°ì¶œë¬¼
- `notebooks/03_feature_engineering.ipynb`
- `pipeline/features/time_series_features.py`
- `pipeline/features/customer_features.py`
- `pipeline/features/composite_features.py`

---

### Week 3: ëª¨ë¸ ê°œë°œ (XGBoost/LightGBM) (12/02 ~ 12/08)

#### ëª©í‘œ
- XGBoost/LightGBM ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•
- ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
- ì´ˆê¸° ì„±ëŠ¥ í‰ê°€

#### ì‘ì—… ë‚´ì—­

1. **ë°ì´í„° ë¶„í• **
   ```python
   # ì‹œê°„ ê¸°ë°˜ ë¶„í• 
   train = df[df['TA_YM'] <= '202406']
   valid = df[(df['TA_YM'] > '202406') & (df['TA_YM'] <= '202409')]
   test = df[df['TA_YM'] > '202409']

   # íŠ¹ì„±/íƒ€ê²Ÿ ë¶„ë¦¬
   feature_cols = [col for col in train.columns if col not in
                   ['ENCODED_MCT', 'TA_YM', 'is_closed', 'MCT_ME_D', ...]]
   X_train, y_train = train[feature_cols], train['is_closed']
   ```

2. **ë¶ˆê· í˜• ì²˜ë¦¬**
   ```python
   # SMOTE Over-sampling
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(sampling_strategy=0.3, random_state=42)
   X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

   # Class Weight ê³„ì‚°
   scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
   ```

3. **XGBoost í•™ìŠµ**
   ```python
   import xgboost as xgb

   xgb_model = xgb.XGBClassifier(
       objective='binary:logistic',
       eval_metric='aucpr',
       scale_pos_weight=scale_pos_weight,
       max_depth=6,
       learning_rate=0.05,
       n_estimators=1000,
       subsample=0.8,
       colsample_bytree=0.8,
       gamma=1,
       random_state=42
   )

   xgb_model.fit(
       X_train, y_train,
       eval_set=[(X_valid, y_valid)],
       early_stopping_rounds=50,
       verbose=50
   )
   ```

4. **LightGBM í•™ìŠµ**
   ```python
   import lightgbm as lgb

   lgb_model = lgb.LGBMClassifier(
       objective='binary',
       metric='auc',
       is_unbalance=True,
       max_depth=6,
       learning_rate=0.05,
       n_estimators=1000,
       subsample=0.8,
       colsample_bytree=0.8,
       min_child_samples=20,
       random_state=42
   )

   lgb_model.fit(
       X_train, y_train,
       eval_set=[(X_valid, y_valid)],
       callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]
   )
   ```

5. **í‰ê°€**
   ```python
   from sklearn.metrics import classification_report, precision_recall_curve, auc

   # ì˜ˆì¸¡
   y_pred_xgb = xgb_model.predict(X_valid)
   y_proba_xgb = xgb_model.predict_proba(X_valid)[:, 1]

   # PR-AUC
   precision, recall, _ = precision_recall_curve(y_valid, y_proba_xgb)
   pr_auc = auc(recall, precision)

   # Classification Report
   print(classification_report(y_valid, y_pred_xgb))
   ```

#### ì‚°ì¶œë¬¼
- `notebooks/04_model_xgboost.ipynb`
- `notebooks/05_model_lightgbm.ipynb`
- `pipeline/models/xgboost_model.py`
- `pipeline/models/lightgbm_model.py`
- `models/xgb_baseline.pkl`
- `models/lgb_baseline.pkl`

---

### Week 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì•™ìƒë¸” (12/09 ~ 12/15)

#### ëª©í‘œ
- Grid Search / Optuna íŠœë‹
- ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•
- ìµœì¢… ì„±ëŠ¥ ìµœì í™”

#### ì‘ì—… ë‚´ì—­

1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna)**
   ```python
   import optuna

   def objective(trial):
       params = {
           'max_depth': trial.suggest_int('max_depth', 3, 10),
           'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
           'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
           'subsample': trial.suggest_float('subsample', 0.6, 1.0),
           'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
           'gamma': trial.suggest_float('gamma', 0, 5),
           'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
           'scale_pos_weight': scale_pos_weight
       }

       model = xgb.XGBClassifier(**params, random_state=42)
       model.fit(X_train, y_train,
                 eval_set=[(X_valid, y_valid)],
                 early_stopping_rounds=50,
                 verbose=False)

       y_proba = model.predict_proba(X_valid)[:, 1]
       precision, recall, _ = precision_recall_curve(y_valid, y_proba)
       pr_auc = auc(recall, precision)

       return pr_auc

   study = optuna.create_study(direction='maximize')
   study.optimize(objective, n_trials=100)
   best_params = study.best_params
   ```

2. **ì‹œê³„ì—´ êµì°¨ ê²€ì¦**
   ```python
   from sklearn.model_selection import TimeSeriesSplit

   tscv = TimeSeriesSplit(n_splits=5, test_size=3)
   pr_aucs = []

   for train_idx, val_idx in tscv.split(X):
       X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
       y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]

       model.fit(X_tr, y_tr)
       y_proba = model.predict_proba(X_val)[:, 1]

       precision, recall, _ = precision_recall_curve(y_val, y_proba)
       pr_aucs.append(auc(recall, precision))

   print(f"Average PR-AUC: {np.mean(pr_aucs):.4f} Â± {np.std(pr_aucs):.4f}")
   ```

3. **ì•™ìƒë¸” - Voting Classifier**
   ```python
   from sklearn.ensemble import VotingClassifier

   voting_clf = VotingClassifier(
       estimators=[
           ('xgb', xgb_model),
           ('lgb', lgb_model)
       ],
       voting='soft',
       weights=[0.6, 0.4]  # ê²€ì¦ ì„±ëŠ¥ ê¸°ë°˜
   )

   voting_clf.fit(X_train, y_train)
   ```

4. **ì•™ìƒë¸” - Stacking**
   ```python
   from sklearn.ensemble import StackingClassifier
   from sklearn.linear_model import LogisticRegression

   stacking_clf = StackingClassifier(
       estimators=[
           ('xgb', xgb_model),
           ('lgb', lgb_model)
       ],
       final_estimator=LogisticRegression(class_weight='balanced'),
       cv=TimeSeriesSplit(n_splits=5)
   )

   stacking_clf.fit(X_train, y_train)
   ```

5. **ì„ê³„ê°’ ìµœì í™”**
   ```python
   # Precision-Recall íŠ¸ë ˆì´ë“œì˜¤í”„
   precision, recall, thresholds = precision_recall_curve(y_valid, y_proba)

   # F2-score ìµœëŒ€í™” (Recall ì¤‘ì‹œ)
   from sklearn.metrics import fbeta_score
   f2_scores = [fbeta_score(y_valid, y_proba >= t, beta=2) for t in thresholds]
   optimal_threshold = thresholds[np.argmax(f2_scores)]

   # ìµœì  ì„ê³„ê°’ ì ìš©
   y_pred_optimal = (y_proba >= optimal_threshold).astype(int)
   ```

#### ì‚°ì¶œë¬¼
- `notebooks/06_hyperparameter_tuning.ipynb`
- `notebooks/07_ensemble.ipynb`
- `pipeline/models/ensemble.py`
- `models/xgb_tuned.pkl`
- `models/lgb_tuned.pkl`
- `models/ensemble_voting.pkl`
- `models/ensemble_stacking.pkl`

---

### Week 5: ëª¨ë¸ í•´ì„ (SHAP) ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ (12/16 ~ 12/22)

#### ëª©í‘œ
- SHAP values ë¶„ì„
- ìœ„ê¸° ì‹ í˜¸ Top 10 ë„ì¶œ
- ì—…ì¢…/ìƒê¶Œë³„ íŒ¨í„´ ë¶„ì„

#### ì‘ì—… ë‚´ì—­

1. **SHAP ë¶„ì„ - ì „ì—­ì  í•´ì„**
   ```python
   import shap

   # SHAP Explainer
   explainer = shap.TreeExplainer(xgb_model)
   shap_values = explainer.shap_values(X_test)

   # Summary Plot (ì „ì²´ íŠ¹ì„± ì¤‘ìš”ë„)
   shap.summary_plot(shap_values, X_test, plot_type='bar')
   shap.summary_plot(shap_values, X_test)

   # Feature Importance
   feature_importance = pd.DataFrame({
       'feature': X_test.columns,
       'shap_importance': np.abs(shap_values).mean(axis=0)
   }).sort_values('shap_importance', ascending=False)

   print("ìœ„ê¸° ì‹ í˜¸ Top 10:")
   print(feature_importance.head(10))
   ```

2. **SHAP ë¶„ì„ - ê°œë³„ ê°€ë§¹ì  í•´ì„**
   ```python
   # íì—… ì˜ˆì¸¡ëœ ê°€ë§¹ì  ì‚¬ë¡€
   high_risk_idx = np.where(y_proba_test > 0.8)[0][0]

   # Waterfall Plot
   shap.waterfall_plot(shap.Explanation(
       values=shap_values[high_risk_idx],
       base_values=explainer.expected_value,
       data=X_test.iloc[high_risk_idx],
       feature_names=X_test.columns.tolist()
   ))

   # Force Plot
   shap.force_plot(
       explainer.expected_value,
       shap_values[high_risk_idx],
       X_test.iloc[high_risk_idx]
   )
   ```

3. **Dependence Plot (ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš©)**
   ```python
   # ë§¤ì¶œ ëŒ€ë¹„ ì¬ë°©ë¬¸ìœ¨ ìƒí˜¸ì‘ìš©
   shap.dependence_plot(
       'M1_SME_RY_SAA_RAT',
       shap_values,
       X_test,
       interaction_index='MCT_UE_CLN_REU_RAT'
   )
   ```

4. **ì—…ì¢…ë³„ ìœ„í—˜ íŒ¨í„´ ë¶„ì„**
   ```python
   # ì—…ì¢…ë³„ ì£¼ìš” ìœ„í—˜ ìš”ì¸
   for industry in df['HPSN_MCT_ZCD_NM'].unique():
       industry_data = df[df['HPSN_MCT_ZCD_NM'] == industry]
       industry_idx = industry_data.index

       industry_shap = shap_values[industry_idx]
       top_features = np.abs(industry_shap).mean(axis=0).argsort()[-5:]

       print(f"\n{industry} ì—…ì¢… ìœ„í—˜ ìš”ì¸:")
       print(X_test.columns[top_features])
   ```

5. **ìœ„í—˜ ìœ í˜• ë¶„ë¥˜**
   ```python
   def classify_risk_type(shap_row, feature_names):
       top_risk = feature_names[np.abs(shap_row).argsort()[-3:]]

       if 'sales_' in ' '.join(top_risk):
           return 'ë§¤ì¶œ ê¸‰ë½í˜•'
       elif 'customer_' in ' '.join(top_risk) or 'revisit' in ' '.join(top_risk):
           return 'ê³ ê° ì´íƒˆí˜•'
       elif 'delivery' in ' '.join(top_risk):
           return 'ë°°ë‹¬ ì˜ì¡´í˜•'
       elif 'rank_' in ' '.join(top_risk):
           return 'ê²½ìŸ ì—´ìœ„í˜•'
       else:
           return 'ì¢…í•© ìœ„ê¸°í˜•'

   df_test['risk_type'] = [
       classify_risk_type(shap_values[i], X_test.columns)
       for i in range(len(shap_values))
   ]

   print(df_test['risk_type'].value_counts())
   ```

6. **Partial Dependence Plot**
   ```python
   from sklearn.inspection import partial_dependence, PartialDependenceDisplay

   features = ['M1_SME_RY_SAA_RAT', 'MCT_UE_CLN_REU_RAT', 'rank_change_industry']

   fig, ax = plt.subplots(figsize=(14, 4))
   PartialDependenceDisplay.from_estimator(
       xgb_model, X_test, features, ax=ax
   )
   plt.tight_layout()
   plt.show()
   ```

#### ì‚°ì¶œë¬¼
- `notebooks/08_shap_analysis.ipynb`
- `notebooks/09_insights.ipynb`
- `pipeline/visualization/shap_plots.py`
- `results/feature_importance.csv`
- `results/risk_patterns_by_industry.csv`
- `results/shap_summary.png`
- `results/risk_type_classification.csv`

---

### Week 6: ë¹„ì¦ˆë‹ˆìŠ¤ ì œì•ˆ ë° ê²°ê³¼ë¬¼ ì •ë¦¬ (12/23 ~ 12/29)

#### ëª©í‘œ
- ê¸ˆìœµìƒí’ˆ ë§¤ì¹­
- ìµœì¢… ê²°ê³¼ë¬¼ ì •ë¦¬
- ë°œí‘œ ìë£Œ ì¤€ë¹„

#### ì‘ì—… ë‚´ì—­

1. **ìœ„í—˜ ìœ í˜•ë³„ ê¸ˆìœµìƒí’ˆ ë§¤ì¹­**
   ```python
   # ìœ„í—˜ ìœ í˜•ë³„ ë§ì¶¤ ì†”ë£¨ì…˜
   risk_solution_map = {
       'ë§¤ì¶œ ê¸‰ë½í˜•': {
           'product': 'ë§ˆì¼€íŒ… ì§€ì› ëŒ€ì¶œ',
           'strategy': [
               'ì˜¨ë¼ì¸ ë§ˆì¼€íŒ… ë¹„ìš© ì§€ì›',
               'ë°°ë‹¬ í”Œë«í¼ ì…ì  ì§€ì›',
               'í”„ë¡œëª¨ì…˜ ìš´ì˜ ìê¸ˆ'
           ],
           'expected_effect': 'ì‹ ê·œ ê³ ê° ìœ ì… ì¦ê°€, ë§¤ì¶œ 15% íšŒë³µ'
       },
       'ê³ ê° ì´íƒˆí˜•': {
           'product': 'ê³ ê° ë¦¬í…ì…˜ ì»¨ì„¤íŒ… + CRM ì‹œìŠ¤í…œ',
           'strategy': [
               'ì¬ë°©ë¬¸ ê³ ê° í¬ì¸íŠ¸ ì ë¦½',
               'ë‹¨ê³¨ ê³ ê° ì „ìš© í˜œíƒ',
               'VIP ê³ ê° ê´€ë¦¬ í”„ë¡œê·¸ë¨'
           ],
           'expected_effect': 'ì¬ë°©ë¬¸ìœ¨ 20% í–¥ìƒ'
       },
       'ë°°ë‹¬ ì˜ì¡´í˜•': {
           'product': 'ì˜¤í”„ë¼ì¸ ê°•í™” í”„ë¡œê·¸ë¨',
           'strategy': [
               'ë§¤ì¥ ë¦¬ë‰´ì–¼ ì§€ì› ëŒ€ì¶œ',
               'í™€ ì„œë¹„ìŠ¤ ê°œì„  ì»¨ì„¤íŒ…',
               'í¬ì¥ ìš©ê¸° ê°œì„  ì§€ì›'
           ],
           'expected_effect': 'ì˜¤í”„ë¼ì¸ ë§¤ì¶œ ë¹„ì¤‘ 30% ì¦ê°€'
       },
       'ê²½ìŸ ì—´ìœ„í˜•': {
           'product': 'ê²½ìŸë ¥ ê°•í™” íŒ¨í‚¤ì§€',
           'strategy': [
               'ë©”ë‰´ ì°¨ë³„í™” ì»¨ì„¤íŒ…',
               'ê°€ê²© ê²½ìŸë ¥ ë¶„ì„',
               'ì›ê°€ ì ˆê° ë°©ì•ˆ ì œì‹œ'
           ],
           'expected_effect': 'ì—…ì¢… ë‚´ ìˆœìœ„ 20% ìƒìŠ¹'
       },
       'ì¢…í•© ìœ„ê¸°í˜•': {
           'product': 'ê²½ì˜ ì•ˆì •í™” íŠ¹ë³„ íŒ¨í‚¤ì§€',
           'strategy': [
               'ê¸´ê¸‰ ìš´ì˜ ìê¸ˆ ì§€ì›',
               'ì „ë¬¸ê°€ 1:1 ì»¨ì„¤íŒ…',
               'ì—…ì¢… ì „í™˜ ì§€ì›'
           ],
           'expected_effect': 'íì—… ìœ„í—˜ 50% ê°ì†Œ'
       }
   }

   # ê°€ë§¹ì ë³„ ì¶”ì²œ ìƒí’ˆ
   df_recommendation = df_test[df_test['y_pred'] == 1].copy()
   df_recommendation['recommended_product'] = df_recommendation['risk_type'].map(
       lambda x: risk_solution_map[x]['product']
   )
   ```

2. **ROI ì˜ˆì¸¡**
   ```python
   # ê°œì… íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜
   def calculate_roi(risk_type, avg_monthly_sales):
       solution = risk_solution_map[risk_type]

       # ë¹„ìš© (í‰ê· )
       cost = {
           'ë§ˆì¼€íŒ… ì§€ì› ëŒ€ì¶œ': 500_000,
           'ê³ ê° ë¦¬í…ì…˜ ì»¨ì„¤íŒ… + CRM ì‹œìŠ¤í…œ': 300_000,
           'ì˜¤í”„ë¼ì¸ ê°•í™” í”„ë¡œê·¸ë¨': 1_000_000,
           'ê²½ìŸë ¥ ê°•í™” íŒ¨í‚¤ì§€': 700_000,
           'ê²½ì˜ ì•ˆì •í™” íŠ¹ë³„ íŒ¨í‚¤ì§€': 2_000_000
       }[solution['product']]

       # ì˜ˆìƒ ë§¤ì¶œ ì¦ê°€ (6ê°œì›”)
       expected_increase = {
           'ë§¤ì¶œ ê¸‰ë½í˜•': avg_monthly_sales * 0.15 * 6,
           'ê³ ê° ì´íƒˆí˜•': avg_monthly_sales * 0.10 * 6,
           'ë°°ë‹¬ ì˜ì¡´í˜•': avg_monthly_sales * 0.12 * 6,
           'ê²½ìŸ ì—´ìœ„í˜•': avg_monthly_sales * 0.08 * 6,
           'ì¢…í•© ìœ„ê¸°í˜•': avg_monthly_sales * 0.20 * 6  # íì—… íšŒí”¼
       }[risk_type]

       roi = (expected_increase - cost) / cost * 100
       return roi

   df_recommendation['estimated_roi'] = df_recommendation.apply(
       lambda row: calculate_roi(row['risk_type'], row['avg_sales']),
       axis=1
   )
   ```

3. **ì¡°ê¸° ê²½ë³´ ì„±ê³µ ì‚¬ë¡€**
   ```python
   # ì‹¤ì œ íì—… ê°€ë§¹ì  ì¤‘ ì˜ˆì¸¡ ì„±ê³µ ì‚¬ë¡€
   success_cases = df_test[
       (df_test['is_closed'] == 1) &
       (df_test['y_pred'] == 1)
   ].copy()

   # Lead time ë¶„ì„
   success_cases['lead_time_days'] = (
       success_cases['MCT_ME_D'] -
       pd.to_datetime(success_cases['TA_YM'], format='%Y%m')
   ).dt.days

   print(f"ì¡°ê¸° ê²½ë³´ ì„±ê³µë¥ : {len(success_cases) / df_test['is_closed'].sum():.2%}")
   print(f"í‰ê·  ë¦¬ë“œ íƒ€ì„: {success_cases['lead_time_days'].mean():.0f}ì¼")

   # ì‚¬ë¡€ ì •ë¦¬
   for idx, case in success_cases.head(5).iterrows():
       print(f"\n[ì‚¬ë¡€ {idx}]")
       print(f"ì—…ì¢…: {case['HPSN_MCT_ZCD_NM']}")
       print(f"ìœ„í—˜ ìœ í˜•: {case['risk_type']}")
       print(f"ì£¼ìš” ìœ„í—˜ ì‹ í˜¸: {case['top_risk_signals']}")
       print(f"ë¦¬ë“œ íƒ€ì„: {case['lead_time_days']}ì¼")
   ```

4. **ìµœì¢… ë³´ê³ ì„œ ì‘ì„±**
   ```markdown
   # ê°€ë§¹ì  ìœ„ê¸° ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ ìµœì¢… ë³´ê³ ì„œ

   ## 1. Executive Summary
   - ëª¨ë¸ ì„±ëŠ¥: PR-AUC 0.XX, Recall@10% 0.XX
   - ì¡°ê¸° ê²½ë³´ ì„±ê³µë¥ : XX%
   - í‰ê·  ë¦¬ë“œ íƒ€ì„: XXì¼

   ## 2. ì£¼ìš” ìœ„ê¸° ì‹ í˜¸ Top 10
   1. ì¬ë°©ë¬¸ìœ¨ 3ê°œì›” ì—°ì† í•˜ë½
   2. ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ 20% í•˜ë½
   ...

   ## 3. ì—…ì¢…ë³„ ìœ„í—˜ íŒ¨í„´
   - ì¹˜í‚¨: ë°°ë‹¬ ì˜ì¡´ë„ ì¦ê°€ + ê°ë‹¨ê°€ í•˜ë½
   - ì¹´í˜: ì‹ ê·œ ê³ ê° ìœ ì… ê°ì†Œ + ê²½ìŸ ì‹¬í™”
   ...

   ## 4. ë§ì¶¤í˜• ê¸ˆìœµìƒí’ˆ ì œì•ˆ
   ...

   ## 5. ê¸°ëŒ€íš¨ê³¼
   - íì—… ì˜ˆë°©: ì—°ê°„ XXê°œ ê°€ë§¹ì 
   - ê²½ì œì  íš¨ê³¼: XXì–µ ì›
   ```

5. **ë°œí‘œ ìë£Œ (PPT)**
   - ë¬¸ì œ ì •ì˜ ë° ë°ì´í„° ë¶„ì„
   - ëª¨ë¸ ì•„í‚¤í…ì²˜ ë° ì„±ëŠ¥
   - SHAP ë¶„ì„ ê²°ê³¼ (ì¸ì‚¬ì´íŠ¸)
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì œì•ˆ
   - ê¸°ëŒ€íš¨ê³¼ ë° í–¥í›„ ê³„íš

#### ì‚°ì¶œë¬¼
- `notebooks/10_business_proposal.ipynb`
- `reports/final_report.pdf`
- `reports/presentation.pptx`
- `results/recommendation_by_merchant.csv`
- `results/roi_simulation.csv`
- `results/success_cases.csv`

---

## 4. ê¸°ìˆ  ìŠ¤íƒ

### 4.1 Core Libraries

```python
# ë°ì´í„° ì²˜ë¦¬
import pandas as pd
import numpy as np
from datetime import datetime

# ëª¨ë¸ë§
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier  # ì˜µì…˜

# ë¶ˆê· í˜• ì²˜ë¦¬
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import TomekLinks
from imblearn.combine import SMOTETomek

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
import optuna
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit

# ì•™ìƒë¸”
from sklearn.ensemble import VotingClassifier, StackingClassifier

# í‰ê°€
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    precision_recall_curve,
    auc,
    fbeta_score,
    roc_auc_score
)

# í•´ì„
import shap
from lime import lime_tabular
from sklearn.inspection import partial_dependence, PartialDependenceDisplay

# ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# ìœ í‹¸ë¦¬í‹°
from scipy.stats import entropy
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib
import pickle
```

### 4.2 ê°œë°œ í™˜ê²½

```toml
# pyproject.toml
[project]
name = "shinhan-202510"
version = "1.0.0"
requires-python = ">=3.12"

dependencies = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "xgboost>=2.0.0",
    "lightgbm>=4.0.0",
    "catboost>=1.2.0",
    "imbalanced-learn>=0.11.0",
    "optuna>=3.3.0",
    "shap>=0.43.0",
    "lime>=0.2.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "plotly>=5.17.0",
    "scipy>=1.11.0",
    "jupyter>=1.0.0",
    "notebook>=7.0.0",
    "ipywidgets>=8.1.0"
]
```

---

## 5. êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 5.1 ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ì „ëµ

#### ì „ëµ ë¹„êµ

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì¶”ì²œ |
|------|------|------|------|
| **SMOTE** | ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ìƒì„± | ê³¼ì í•© ìœ„í—˜ | â­ |
| **ADASYN** | ê²½ê³„ ì˜ì—­ ì§‘ì¤‘ ìƒì„± | ë…¸ì´ì¦ˆ ë¯¼ê° | â–³ |
| **Tomek Links** | ê²½ê³„ ì •ë¦¬ | ë°ì´í„° ì†ì‹¤ | - |
| **SMOTETomek** | í•˜ì´ë¸Œë¦¬ë“œ | ë³µì¡ì„± ì¦ê°€ | â­ |
| **Class Weight** | ê°„ë‹¨, ë¹ ë¦„ | ì„±ëŠ¥ ì œí•œì  | â­ |

#### êµ¬í˜„ ì½”ë“œ

```python
# 1. SMOTE (ì¶”ì²œ)
from imblearn.over_sampling import SMOTE

smote = SMOTE(
    sampling_strategy=0.3,  # ì†Œìˆ˜:ë‹¤ìˆ˜ = 3:10
    k_neighbors=5,
    random_state=42
)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print(f"Before SMOTE: {y_train.value_counts()}")
print(f"After SMOTE: {y_train_sm.value_counts()}")

# 2. Class Weight (XGBoost/LightGBM)
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

xgb_params = {
    'scale_pos_weight': scale_pos_weight,  # ~32
    ...
}

lgb_params = {
    'is_unbalance': True,
    # ë˜ëŠ” 'class_weight': 'balanced'
    ...
}

# 3. SMOTETomek (ê³ ê¸‰)
from imblearn.combine import SMOTETomek

smt = SMOTETomek(random_state=42)
X_train_smt, y_train_smt = smt.fit_resample(X_train, y_train)
```

### 5.2 ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

#### Data Leakage ë°©ì§€

```python
# âŒ ì˜ëª»ëœ ì˜ˆ (ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ)
df['sales_ma_3m'] = df['sales'].rolling(3).mean()  # ë¯¸ë˜ ë°ì´í„° í¬í•¨

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ (ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
df['sales_ma_3m'] = df.groupby('ENCODED_MCT')['sales'].transform(
    lambda x: x.shift(1).rolling(3, min_periods=1).mean()
)

# ë˜ëŠ”
df['sales_lag_1'] = df.groupby('ENCODED_MCT')['sales'].shift(1)
df['sales_ma_3m'] = df['sales_lag_1'].rolling(3, min_periods=1).mean()
```

#### íš¨ìœ¨ì ì¸ íŠ¹ì„± ìƒì„±

```python
def create_time_features(df, target_col, mct_col='ENCODED_MCT'):
    """ì‹œê³„ì—´ íŠ¹ì„± ì¼ê´„ ìƒì„±"""

    features = df.copy()

    # Lag features
    for lag in [1, 3, 6]:
        features[f'{target_col}_lag_{lag}'] = features.groupby(mct_col)[target_col].shift(lag)

    # Rolling statistics
    for window in [3, 6]:
        features[f'{target_col}_ma_{window}'] = features.groupby(mct_col)[target_col].transform(
            lambda x: x.shift(1).rolling(window, min_periods=1).mean()
        )
        features[f'{target_col}_std_{window}'] = features.groupby(mct_col)[target_col].transform(
            lambda x: x.shift(1).rolling(window, min_periods=1).std()
        )

    # Change rates
    features[f'{target_col}_change_1m'] = features.groupby(mct_col)[target_col].pct_change(1)
    features[f'{target_col}_change_3m'] = features.groupby(mct_col)[target_col].pct_change(3)

    # Consecutive patterns
    features[f'{target_col}_consecutive_up'] = features.groupby(mct_col)[f'{target_col}_change_1m'].apply(
        lambda x: (x > 0).astype(int).groupby((x <= 0).cumsum()).cumsum()
    )
    features[f'{target_col}_consecutive_down'] = features.groupby(mct_col)[f'{target_col}_change_1m'].apply(
        lambda x: (x < 0).astype(int).groupby((x >= 0).cumsum()).cumsum()
    )

    return features

# ì‚¬ìš© ì˜ˆ
df = create_time_features(df, 'RC_M1_SAA')
df = create_time_features(df, 'RC_M1_UE_CUS_CN')
```

### 5.3 ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦

#### Early Stopping í™œìš©

```python
# XGBoost
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_valid, y_valid)],
    eval_metric=['aucpr', 'logloss'],
    early_stopping_rounds=50,
    verbose=50
)

# ìµœì  iteration í™•ì¸
best_iteration = xgb_model.best_iteration
print(f"Best iteration: {best_iteration}")

# LightGBM
lgb_model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_valid, y_valid)],
    eval_metric='auc',
    callbacks=[
        lgb.early_stopping(50),
        lgb.log_evaluation(50)
    ]
)
```

#### ì‹œê³„ì—´ êµì°¨ ê²€ì¦

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5, test_size=3)  # 3ê°œì›”ì”© ê²€ì¦

cv_scores = []
for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
    print(f"\nFold {fold + 1}")

    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]

    model = xgb.XGBClassifier(**best_params)
    model.fit(X_tr, y_tr,
              eval_set=[(X_val, y_val)],
              early_stopping_rounds=30,
              verbose=False)

    y_proba = model.predict_proba(X_val)[:, 1]
    precision, recall, _ = precision_recall_curve(y_val, y_proba)
    pr_auc = auc(recall, precision)

    cv_scores.append(pr_auc)
    print(f"PR-AUC: {pr_auc:.4f}")

print(f"\nAverage PR-AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
```

### 5.4 ì•™ìƒë¸” ì „ëµ

#### Weighted Voting

```python
# ê²€ì¦ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
xgb_score = pr_auc_xgb  # ì˜ˆ: 0.65
lgb_score = pr_auc_lgb  # ì˜ˆ: 0.63

total_score = xgb_score + lgb_score
xgb_weight = xgb_score / total_score  # 0.51
lgb_weight = lgb_score / total_score  # 0.49

# Voting Classifier
voting_clf = VotingClassifier(
    estimators=[
        ('xgb', xgb_model),
        ('lgb', lgb_model)
    ],
    voting='soft',
    weights=[xgb_weight, lgb_weight]
)

voting_clf.fit(X_train, y_train)
y_proba_ensemble = voting_clf.predict_proba(X_test)[:, 1]
```

#### Stacking

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Meta-learnerì— í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
meta_learner = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    random_state=42
)

stacking_clf = StackingClassifier(
    estimators=[
        ('xgb', xgb_model),
        ('lgb', lgb_model)
    ],
    final_estimator=meta_learner,
    cv=TimeSeriesSplit(n_splits=5),
    passthrough=False  # Base model ì˜ˆì¸¡ë§Œ ì‚¬ìš©
)

stacking_clf.fit(X_train, y_train)
```

---

## 6. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
shinhan_202510/
â”‚
â”œâ”€â”€ data/                           # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ big_data_set1_f.csv
â”‚   â”œâ”€â”€ big_data_set2_f.csv
â”‚   â””â”€â”€ big_data_set3_f.csv
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ 00_temp.ipynb
â”‚   â”œâ”€â”€ 01_eda.ipynb               # EDA (ì™„ë£Œ)
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb     # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 04_model_xgboost.ipynb
â”‚   â”œâ”€â”€ 05_model_lightgbm.ipynb
â”‚   â”œâ”€â”€ 06_hyperparameter_tuning.ipynb
â”‚   â”œâ”€â”€ 07_ensemble.ipynb
â”‚   â”œâ”€â”€ 08_shap_analysis.ipynb
â”‚   â”œâ”€â”€ 09_insights.ipynb
â”‚   â””â”€â”€ 10_business_proposal.ipynb
â”‚
â”œâ”€â”€ pipeline/                      # ML íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py        # ë°ì´í„° ë¡œë“œ
â”‚   â”‚   â”œâ”€â”€ feature_encoder.py    # êµ¬ê°„ ì¸ì½”ë”©
â”‚   â”‚   â””â”€â”€ missing_handler.py    # ê²°ì¸¡ê°’ ì²˜ë¦¬
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ time_series_features.py
â”‚   â”‚   â”œâ”€â”€ customer_features.py
â”‚   â”‚   â”œâ”€â”€ composite_features.py
â”‚   â”‚   â””â”€â”€ feature_selector.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”‚   â”œâ”€â”€ lightgbm_model.py
â”‚   â”‚   â”œâ”€â”€ ensemble.py
â”‚   â”‚   â””â”€â”€ model_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py            # Custom metrics
â”‚   â”‚   â””â”€â”€ validators.py         # CV ì „ëµ
â”‚   â”‚
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ shap_plots.py
â”‚       â”œâ”€â”€ performance_plots.py
â”‚       â””â”€â”€ business_plots.py
â”‚
â”œâ”€â”€ models/                        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
â”‚   â”œâ”€â”€ xgb_baseline.pkl
â”‚   â”œâ”€â”€ lgb_baseline.pkl
â”‚   â”œâ”€â”€ xgb_tuned.pkl
â”‚   â”œâ”€â”€ lgb_tuned.pkl
â”‚   â”œâ”€â”€ ensemble_voting.pkl
â”‚   â”œâ”€â”€ ensemble_stacking.pkl
â”‚   â””â”€â”€ best_model.pkl
â”‚
â”œâ”€â”€ results/                       # ë¶„ì„ ê²°ê³¼
â”‚   â”œâ”€â”€ feature_importance.csv
â”‚   â”œâ”€â”€ shap_values.npy
â”‚   â”œâ”€â”€ risk_patterns_by_industry.csv
â”‚   â”œâ”€â”€ risk_type_classification.csv
â”‚   â”œâ”€â”€ recommendation_by_merchant.csv
â”‚   â”œâ”€â”€ roi_simulation.csv
â”‚   â”œâ”€â”€ success_cases.csv
â”‚   â””â”€â”€ visualizations/
â”‚       â”œâ”€â”€ shap_summary.png
â”‚       â”œâ”€â”€ pr_curve.png
â”‚       â””â”€â”€ confusion_matrix.png
â”‚
â”œâ”€â”€ reports/                       # ìµœì¢… ë³´ê³ ì„œ
â”‚   â”œâ”€â”€ final_report.pdf
â”‚   â”œâ”€â”€ presentation.pptx
â”‚   â””â”€â”€ executive_summary.md
â”‚
â”œâ”€â”€ docs/                          # ë¬¸ì„œ
â”‚   â”œâ”€â”€ 00_bigcontest_2025.md
â”‚   â”œâ”€â”€ 01_data_layout.md
â”‚   â”œâ”€â”€ 02_approach.md
â”‚   â””â”€â”€ 03_plan_XGBoost_LightGBM.md  # ì´ ë¬¸ì„œ
â”‚
â”œâ”€â”€ configs/                       # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ model_params.yaml
â”‚
â”œâ”€â”€ tests/                         # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_features.py
â”‚   â””â”€â”€ test_models.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ main.py
```

---

## 7. ì˜ˆìƒ ê²°ê³¼ë¬¼

### 7.1 ëª¨ë¸ ì„±ëŠ¥ ëª©í‘œ

| ì§€í‘œ | ëª©í‘œê°’ | ë‹¬ì„± ì „ëµ |
|------|--------|-----------|
| **PR-AUC** | > 0.55 | ì•™ìƒë¸” + ì‹œê³„ì—´ íŠ¹ì„± |
| **Recall@10%** | > 0.70 | ë¶ˆê· í˜• ì²˜ë¦¬ + ì„ê³„ê°’ ìµœì í™” |
| **Precision@10%** | > 0.60 | Feature Engineering |
| **F2-score** | > 0.60 | Recall ì¤‘ì‹œ íŠœë‹ |
| **í‰ê·  ë¦¬ë“œ íƒ€ì„** | > 60ì¼ | ì¡°ê¸° íƒ€ê²Ÿ ë³€ìˆ˜ (3ê°œì›” ì „) |

### 7.2 í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (ì˜ˆìƒ)

#### ì£¼ìš” ìœ„ê¸° ì‹ í˜¸ Top 10
1. **ì¬ë°©ë¬¸ìœ¨ 3ê°œì›” ì—°ì† í•˜ë½** (SHAP: 0.25)
2. **ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ 20% í•˜ë½** (SHAP: 0.18)
3. **ì‹ ê·œ ê³ ê° ìœ ì… 6ê°œì›” ê°ì†Œ** (SHAP: 0.15)
4. **ë°°ë‹¬ ì˜ì¡´ë„ ê¸‰ì¦ + ì´ ë§¤ì¶œ ê°ì†Œ** (SHAP: 0.12)
5. **ê°ë‹¨ê°€ ì§€ì† í•˜ë½** (SHAP: 0.10)
6. **ìƒê¶Œ ë‚´ í•´ì§€ìœ¨ ì¦ê°€** (SHAP: 0.09)
7. **ë§¤ì¶œ ë³€ë™ì„± ê¸‰ì¦** (SHAP: 0.08)
8. **ìš´ì˜ ì´ˆê¸° 6ê°œì›” + ë§¤ì¶œ ë¶€ì§„** (SHAP: 0.07)
9. **ê³ ê° ë‹¤ì–‘ì„± ê°ì†Œ** (SHAP: 0.06)
10. **ìœ ë™ì¸êµ¬ ê³ ê° ë¹„ì¤‘ ê¸‰ê°** (SHAP: 0.05)

#### ì—…ì¢…ë³„ ìœ„í—˜ íŒ¨í„´
- **ì¹˜í‚¨**: ë°°ë‹¬ ì˜ì¡´ë„ â†‘ + ê°ë‹¨ê°€ â†“
- **ì¹´í˜**: ì‹ ê·œ ê³ ê° ìœ ì… â†“ + ê²½ìŸ ì‹¬í™”
- **í•œì‹**: ì¬ë°©ë¬¸ìœ¨ â†“ + ê³ ê° ê³ ë ¹í™”
- **ì¼ì‹**: ê³ ê°€ ë©”ë‰´ íŒë§¤ â†“ + ë³€ë™ì„± â†‘

### 7.3 ë¹„ì¦ˆë‹ˆìŠ¤ ì œì•ˆ

#### ìœ„í—˜ ìœ í˜•ë³„ ê¸ˆìœµìƒí’ˆ

| ìœ„í—˜ ìœ í˜• | ì¶”ì²œ ìƒí’ˆ | ê¸°ëŒ€íš¨ê³¼ | ì˜ˆìƒ ROI |
|-----------|----------|----------|----------|
| ë§¤ì¶œ ê¸‰ë½í˜• | ë§ˆì¼€íŒ… ì§€ì› ëŒ€ì¶œ | ë§¤ì¶œ 15% íšŒë³µ | 180% |
| ê³ ê° ì´íƒˆí˜• | CRM ì‹œìŠ¤í…œ ì§€ì› | ì¬ë°©ë¬¸ìœ¨ 20% â†‘ | 150% |
| ë°°ë‹¬ ì˜ì¡´í˜• | ì˜¤í”„ë¼ì¸ ê°•í™” í”„ë¡œê·¸ë¨ | ì˜¤í”„ë¼ì¸ ë§¤ì¶œ 30% â†‘ | 120% |
| ê²½ìŸ ì—´ìœ„í˜• | ê²½ìŸë ¥ ê°•í™” íŒ¨í‚¤ì§€ | ìˆœìœ„ 20% ìƒìŠ¹ | 140% |
| ì¢…í•© ìœ„ê¸°í˜• | ê²½ì˜ ì•ˆì •í™” íŒ¨í‚¤ì§€ | íì—… ìœ„í—˜ 50% â†“ | 250% |

#### ê²½ì œì  íš¨ê³¼ (ì˜ˆìƒ)
- **íì—… ì˜ˆë°©**: ì—°ê°„ 50~80ê°œ ê°€ë§¹ì 
- **ë§¤ì¶œ íšŒë³µ**: ê°€ë§¹ì ë‹¹ í‰ê·  500ë§Œì›/ì›” Ã— 6ê°œì›” = 3,000ë§Œì›
- **ì´ ê²½ì œì  íš¨ê³¼**: 15ì–µ ~ 24ì–µ ì›/ë…„
- **ê¸ˆìœµìƒí’ˆ ëŒ€ì¶œì•¡**: ê°€ë§¹ì ë‹¹ í‰ê·  100ë§Œì› (ì´ 5ì²œë§Œ~8ì²œë§Œì›)
- **ì˜ˆìƒ ìˆ˜ìµ**: ëŒ€ì¶œ ì´ì + ì»¨ì„¤íŒ… ìˆ˜ìˆ˜ë£Œ = 1ì–µ ~ 1.5ì–µ ì›/ë…„

### 7.4 ìµœì¢… ì œì¶œë¬¼

#### 1. ë¶„ì„ ë³´ê³ ì„œ (PDF)
- Executive Summary (2í˜ì´ì§€)
- ë°ì´í„° ë¶„ì„ (10í˜ì´ì§€)
- ëª¨ë¸ë§ ê²°ê³¼ (10í˜ì´ì§€)
- ì¸ì‚¬ì´íŠ¸ ë° ì œì•ˆ (8í˜ì´ì§€)

#### 2. ë°œí‘œ ìë£Œ (PPT)
- ë¬¸ì œ ì •ì˜ (3 slides)
- EDA ë° ì¸ì‚¬ì´íŠ¸ (5 slides)
- ëª¨ë¸ ì•„í‚¤í…ì²˜ (4 slides)
- SHAP ë¶„ì„ ê²°ê³¼ (5 slides)
- ë¹„ì¦ˆë‹ˆìŠ¤ ì œì•ˆ (5 slides)
- Q&A (2 slides)

#### 3. ì½”ë“œ ë° ëª¨ë¸
- GitHub Repository
- ì¬í˜„ ê°€ëŠ¥í•œ Jupyter Notebooks
- í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ (.pkl)
- README with ì‹¤í–‰ ê°€ì´ë“œ

#### 4. ë¶€ë¡
- Feature ëª©ë¡ ë° ì •ì˜
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¡œê·¸
- ì¶”ê°€ ì‹œê°í™” ìë£Œ

---

## 8. ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘ ë°©ì•ˆ

### 8.1 ì ì¬ì  ìœ„í—˜

| ìœ„í—˜ | ì˜í–¥ | í™•ë¥  | ëŒ€ì‘ ë°©ì•ˆ |
|------|------|------|-----------|
| **ë°ì´í„° ë¶ˆê· í˜• ì‹¬í™”** | ì„±ëŠ¥ ì €í•˜ | ë†’ìŒ | SMOTE + Class Weight ë³‘í–‰ |
| **ê³¼ì í•©** | ì¼ë°˜í™” ì‹¤íŒ¨ | ì¤‘ê°„ | Early Stopping + CV |
| **Feature ëˆ„ì¶œ** | ë¹„í˜„ì‹¤ì  ì„±ëŠ¥ | ì¤‘ê°„ | Lag featuresë§Œ ì‚¬ìš© ê²€ì¦ |
| **í•´ì„ ë³µì¡ë„** | ì¸ì‚¬ì´íŠ¸ ë¶€ì¡± | ë‚®ìŒ | SHAP + ë„ë©”ì¸ ì „ë¬¸ê°€ í˜‘ì—… |
| **ê³„ì‚° ì‹œê°„ ë¶€ì¡±** | íŠœë‹ ì œí•œ | ì¤‘ê°„ | Optuna íš¨ìœ¨ì  íƒìƒ‰ |

### 8.2 ëŒ€ì‘ ì „ëµ

1. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**
   - Train/Valid/Test ì„±ëŠ¥ ì§€ì† ì¶”ì 
   - Overfitting ì¡°ê¸° ë°œê²¬

2. **ë°±ì—… í”Œëœ**
   - XGBoost ë‹¨ë… ëª¨ë¸ í™•ë³´
   - ê°„ë‹¨í•œ Voting ì•™ìƒë¸” ëŒ€ì•ˆ

3. **ì‹œê°„ ê´€ë¦¬**
   - ì£¼ì°¨ë³„ ë§ˆì¼ìŠ¤í†¤ ì—„ìˆ˜
   - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‘ì—…

---

## 9. ì„±ê³µ ê¸°ì¤€

### 9.1 ê¸°ìˆ ì  ì„±ê³µ

- [ ] PR-AUC > 0.55
- [ ] Recall@10% > 0.70
- [ ] í‰ê·  ë¦¬ë“œ íƒ€ì„ > 60ì¼
- [ ] SHAP ë¶„ì„ ì™„ë£Œ
- [ ] ì¬í˜„ ê°€ëŠ¥í•œ ì½”ë“œ

### 9.2 ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³µ

- [ ] ìœ„ê¸° ì‹ í˜¸ Top 10 ë„ì¶œ
- [ ] ì—…ì¢…ë³„ íŒ¨í„´ 5ê°œ ì´ìƒ
- [ ] ê¸ˆìœµìƒí’ˆ ë§¤ì¹­ ì²´ê³„ êµ¬ì¶•
- [ ] ROI ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ
- [ ] ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ

### 9.3 í‰ê°€ ê¸°ì¤€ (100ì )

- **ëª¨ë¸ë§ ì í•©ì„±/ì™„ì„±ë„** (25ì ): XGBoost/LightGBM ì•™ìƒë¸”, ë¶ˆê· í˜• ì²˜ë¦¬
- **ë°ì´í„° í™œìš©/ë¶„ì„ë ¥** (25ì ): ì‹œê³„ì—´ íŠ¹ì„±, ë„ë©”ì¸ ì§€ì‹ í™œìš©
- **ì¸ì‚¬ì´íŠ¸/ì‹¤íš¨ì„±** (20ì ): SHAP ë¶„ì„, ìœ„ê¸° ì‹ í˜¸ ë„ì¶œ
- **ê¸ˆìœµìƒí’ˆ ì œì•ˆ** (20ì ): ìœ„í—˜ ìœ í˜•ë³„ ë§ì¶¤ ì†”ë£¨ì…˜
- **ì™„ì„±ë„** (10ì ): ë³´ê³ ì„œ í’ˆì§ˆ, ë°œí‘œ ì™„ì„±ë„

---

## 10. ì°¸ê³  ìë£Œ

### 10.1 ê´€ë ¨ ë…¼ë¬¸
- Chen & Guestrin (2016). "XGBoost: A Scalable Tree Boosting System"
- Ke et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
- Chawla et al. (2002). "SMOTE: Synthetic Minority Over-sampling Technique"
- Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions (SHAP)"

### 10.2 ìœ ìš©í•œ ë§í¬
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [SHAP Documentation](https://shap.readthedocs.io/)
- [Imbalanced-learn Guide](https://imbalanced-learn.org/)
- [Optuna Tutorials](https://optuna.readthedocs.io/)

### 10.3 Kaggle ì°¸ê³  ì†”ë£¨ì…˜
- [Imbalanced Classification - Credit Card Fraud](https://www.kaggle.com/code/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets)
- [Time Series with XGBoost](https://www.kaggle.com/code/robikscube/time-series-forecasting-with-xgboost)
- [SHAP Analysis Tutorial](https://www.kaggle.com/code/dansbecker/shap-values)

---

**ì‘ì„±ì¼**: 2025-10-05
**ë²„ì „**: 1.0
**ì‘ì„±ì**: AI Analysis Team
**í”„ë¡œì íŠ¸**: 2025 ë¹…ì½˜í…ŒìŠ¤íŠ¸ - ê°€ë§¹ì  ìœ„ê¸° ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ
